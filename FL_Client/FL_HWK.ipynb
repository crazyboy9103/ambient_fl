{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jetson Nano를 사용해서 Federated Learning으로 학습 과제\n",
    "* 이전 실습에서 Django를 가지고 서버를 만들고 Federated learning을 구현해보는 실습을 진행하였다.\n",
    "* 이번 과제에서는 Jetson Nano를 사용해서 Federated Learning을 진행해야 한다.\n",
    "\n",
    "* 우선 필요한 패키지를 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement absl-py~=0.10, but you'll have absl-py 0.9.0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement gast==0.4.0, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement grpcio<2.0,>=1.37.0, but you'll have grpcio 1.27.2 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement keras~=2.6, but you'll have keras 2.3.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement keras-preprocessing~=1.1.2, but you'll have keras-preprocessing 1.1.0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement numpy~=1.19.2, but you'll have numpy 1.18.4 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement six~=1.15.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement tensorboard~=2.6, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement tensorflow-estimator~=2.6, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow-gpu 2.6.0 has requirement wheel~=0.35, but you'll have wheel 0.34.2 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jetson_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jetson_client.py\n",
    "# Usage : python jetson_client.py --ip IP --p PORT\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from random import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Quiet tensorflow error messages\n",
    "\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder): # inherits JSONEncoder \n",
    "    def default(self, o):\n",
    "        if isinstance(o, np.ndarray):\n",
    "            return o.tolist()\n",
    "        return json.JSONEncoder.default(self, o)\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, max_round: int, time_delay = 5, suppress=True, num_samples=600):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            experiment : Desired data split type (1~4)\n",
    "            max_round : the maximum number of rounds that should be trained (arbitrary integer)\n",
    "            model : the NN model type (either 'ann' or 'cnn')\n",
    "            time_delay : the time delay until the next local check (arbitrary positive integer) \n",
    "                        (Need to increase this value if one round of training takes much longer than current time_delay. \n",
    "                        The reason is that any network communication until next round after the client has already uploaded \n",
    "                        the parameters for current round increases network overhead. Thus, higher time_delay will make communication\n",
    "                        more stable while increasing the absolute time it takes. Requires careful selection of this value.)\n",
    "            suppress : boolean value to print the logs\n",
    "        \n",
    "        @return: \n",
    "            None : Initializes the variables\n",
    "                   Setup the urls for communication\n",
    "                   Fetch client's id from the server\n",
    "                   Downloads MNIST dataset and splits\n",
    "                   Build model\n",
    "        \"\"\"\n",
    "        base_url = f\"http://{IP}:{PORT}/\" # Base Url that we communicate with\n",
    "        self.weight_url = base_url + \"weight\" # Url that we send or fetch weight parameters\n",
    "        self.round_url = base_url + \"round\" # Url that helps synchronization\n",
    "        self.id_url = base_url+\"get_id\" # Url from which we fetch the current client's id\n",
    "        self.total_num_data_url = base_url + \"total_num\" # Url from which we fetch the number of total data points (seen by N clients)\n",
    "        self.experiment_url = base_url + \"experiment\"\n",
    "        self.accuracy_url = base_url + \"accuracy\"\n",
    "        self.fed_id = self.request_fed_id() \n",
    "        self.experiment = self.request_experiment() # Experiment to test the performance of federated learning regime \n",
    "        \n",
    "        self.time_delay = time_delay\n",
    "        \n",
    "        self.suppress = suppress\n",
    "        '''\n",
    "        Initial setup\n",
    "        '''\n",
    "        self.global_round = self.request_global_round()\n",
    "        self.current_round = 0\n",
    "        \n",
    "        #self.change_client_number(max_round)\n",
    "        self.max_round = max_round # Set the maximum number of rounds\n",
    "        '''\n",
    "        Downloads MNIST dataset and prepares (train_x, train_y), (test_x, test_y)\n",
    "        '''\n",
    "        self.train_images, self.train_labels = None, None\n",
    "        self.test_images, self.test_labels = None, None\n",
    "        self.prepare_images()\n",
    "        \n",
    "        self.train_index_list = None\n",
    "        self.test_index_list = None\n",
    "        self.split_train_images = []\n",
    "        self.split_train_labels = []\n",
    "        \n",
    "        self.local_data_num = 0\n",
    "        self.data_split(num_samples=num_samples)\n",
    "        \n",
    "        '''\n",
    "        Builds model\n",
    "        '''\n",
    "        self.model = None\n",
    "        self.build_cnn_model()\n",
    "        \n",
    "    def prepare_images(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            model : 'ann' or 'cnn'. They need slightly different format for the input. For cnn, we add additional dimension for channel\n",
    "        \n",
    "        @return: \n",
    "            None : Prepares MNIST images in the required format for each model\n",
    "            \n",
    "        \"\"\"\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_images, self.train_labels), (self.test_images, self.test_labels) = mnist.load_data()\n",
    "        self.train_images, self.test_images = self.train_images / 255, self.test_images / 255\n",
    "        \n",
    "        # For CNN, add dummy channel to feed the images to CNN\n",
    "        self.train_images=self.train_images.reshape(-1,28, 28, 1)\n",
    "        self.test_images=self.test_images.reshape(-1,28, 28, 1)\n",
    "            \n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            None : saves the CNN model in self.model variable \n",
    "        \"\"\"\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "    def data_split(self, num_samples):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            num_samples : The number of sample images in each client. This value is used for equally\n",
    "                          sized dataset\n",
    "        \n",
    "        @return: \n",
    "            None : Split the dataset depending on the self.experiment value\n",
    "           \n",
    "                If self.experiment is 1: Uniform data split: We take equal amount of data from each class (iid)\n",
    "                If self.experiment is 2: Random data split1: We take equal amount of data, but not uniformly distributed across classes\n",
    "                If self.experiment is 3: Random data split2: We take different amount of data and not uniformly distributed across classes\n",
    "                If self.experiment is 4: Skewed: We take disproportionate amount of data for some classes\n",
    "                        \n",
    "        \"\"\"\n",
    "        if self.train_index_list is None or self.test_index_list is None:\n",
    "            self.train_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "            self.test_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "            for i, v in enumerate(self.train_labels):\n",
    "                self.train_index_list[v].append(i)\n",
    "\n",
    "            for i, v in enumerate(self.test_labels):\n",
    "                self.test_index_list[v].append(i)\n",
    "\n",
    "        \n",
    "        self.split_train_images = []\n",
    "        self.split_train_labels = []\n",
    "        \n",
    "        if self.experiment == 1: #uniform data split\n",
    "            self.local_data_num = num_samples\n",
    "            \n",
    "\n",
    "\n",
    "            for i in range(len(self.train_index_list)):\n",
    "                indices = self.train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=num_samples//10)\n",
    "                \n",
    "                self.split_train_images.extend(self.train_images[random_indices])\n",
    "                self.split_train_labels.extend(self.train_labels[random_indices])\n",
    "            \n",
    "\n",
    "        elif self.experiment == 2: # Randomly selected, equally sized dataset\n",
    "            self.local_data_num = num_samples\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=num_samples)\n",
    "            self.split_train_images = self.train_images[random_indices]\n",
    "            self.split_train_labels = self.train_labels[random_indices]\n",
    "\n",
    "            counts = [0 for _ in range(10)]\n",
    "            \n",
    "            for label in self.train_labels[random_indices]:\n",
    "                counts[label] += 1\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        elif self.experiment == 3: # Randomly selected, differently sized dataset\n",
    "            n = np.random.randint(1, num_samples)\n",
    "            self.local_data_num = n\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=n)\n",
    "            self.split_train_images = self.train_images[random_indices]\n",
    "            self.split_train_labels = self.train_labels[random_indices]\n",
    "            \n",
    "\n",
    "            counts = [0 for _ in range(10)]\n",
    "            \n",
    "            for label in self.train_labels[random_indices]:\n",
    "                counts[label] += 1\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        elif self.experiment == 4: #Skewed\n",
    "            skewed_numbers = np.random.choice([i for i in range(10)], np.random.randint(1, 10))\n",
    "            non_skewed_numbers = list(set([i for i in range(10)])-set(skewed_numbers))\n",
    "            N = 0\n",
    "            \n",
    "            counts = [0 for _ in range(10)]\n",
    "            \n",
    "            for i in skewed_numbers:\n",
    "                n = np.random.randint(50, 60)\n",
    "                N += n\n",
    "                \n",
    "                indices = self.train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                self.split_train_images.extend(self.train_images[random_indices])\n",
    "                self.split_train_labels.extend(self.train_labels[random_indices])\n",
    "                \n",
    "                counts[i] += n\n",
    "            \n",
    "                \n",
    "            for i in non_skewed_numbers:\n",
    "                n = np.random.randint(1, 10)\n",
    "                N += n\n",
    "                \n",
    "                indices = self.train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                self.split_train_images.extend(self.train_images[random_indices])\n",
    "                self.split_train_labels.extend(self.train_labels[random_indices])\n",
    "                \n",
    "                counts[i] += n\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.local_data_num = N\n",
    "        \n",
    "        else:\n",
    "            print(\"Pick from 1,2,3,4\")\n",
    "            return \n",
    "    \n",
    "        self.split_train_images = np.array(self.split_train_images)\n",
    "        self.split_train_labels = np.array(self.split_train_labels)\n",
    "        \n",
    "        self.update_total_num_data(self.local_data_num)    \n",
    "\n",
    "        \n",
    "        \n",
    "    def update_total_num_data(self, num_data):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            num_data : the number of training images that the current client has\n",
    "        \n",
    "        @return: \n",
    "            None : update the total number of training images that is stored in the server\n",
    "        \"\"\"\n",
    "        local_num_data_to_json = json.dumps(num_data)\n",
    "        requests.put(self.total_num_data_url, data=local_num_data_to_json)\n",
    "    \n",
    "    def request_total_num_data(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            result : Total number of training images available to all clients\n",
    "        \"\"\"\n",
    "        result = requests.get(self.total_num_data_url)\n",
    "        result = int(result.text)\n",
    "        return result\n",
    "\n",
    "    def request_fed_id(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            result : Automatically assigned client id that is given by the server\n",
    "        \"\"\"\n",
    "        result = requests.get(self.id_url)\n",
    "        result = result.json()\n",
    "        return result\n",
    "    \n",
    "    def request_global_round(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            result : Current global round that the server is in\n",
    "        \"\"\"\n",
    "        result = requests.get(self.round_url)\n",
    "        result = result.json()\n",
    "        return result\n",
    "    \n",
    "    def request_experiment(self):\n",
    "        result = requests.get(self.experiment_url)\n",
    "        result_data = result.json()\n",
    "        \n",
    "        if result_data is not None:\n",
    "            return int(result_data)\n",
    "        \n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def request_global_weight(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            global_weight : Up-to-date version of the model parameters\n",
    "        \"\"\"\n",
    "        result = requests.get(self.weight_url)\n",
    "        result_data = result.json()\n",
    "        \n",
    "        global_weight = None\n",
    "        if result_data is not None:\n",
    "            global_weight = []\n",
    "            for i in range(len(result_data)):\n",
    "                temp = np.array(result_data[i], dtype=np.float32)\n",
    "                global_weight.append(temp)\n",
    "            \n",
    "        \n",
    "        return global_weight\n",
    "\n",
    "    def upload_local_weight(self, local_weight=[]):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            local_weight : the local weight that current client has converged to\n",
    "        \n",
    "        @return: \n",
    "            None : Add current client's weights to the server (Server accumulates these from multiple clients and computes the global weight)\n",
    "        \"\"\"\n",
    "        local_weight_to_json = json.dumps(local_weight, cls=NumpyEncoder)\n",
    "        requests.put(self.weight_url, data=local_weight_to_json)\n",
    "        \n",
    "    def upload_local_accuracy(self, accuracy):\n",
    "        temp_dict = {'acc':accuracy, 'id':self.fed_id}\n",
    "        local_acc_to_json = json.dumps(temp_dict)\n",
    "        requests.put(self.accuracy_url, data=local_acc_to_json)\n",
    "        \n",
    "    def validation(self, local_weight=[]):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            local_weight : the current client's weights\n",
    "        \n",
    "        @return: \n",
    "            acc : test accuracy of the current client's model\n",
    "        \"\"\"\n",
    "        if local_weight is not None:\n",
    "            self.model.set_weights(local_weight)\n",
    "            acc = self.model.evaluate(self.test_images, self.test_labels, verbose=0 if self.suppress else 1)\n",
    "            self.upload_local_accuracy(acc)\n",
    "            e = {out: acc[i] for i, out in enumerate(self.model.metrics_names)}\n",
    "\n",
    "            return acc\n",
    "        \n",
    "    def train_local_model(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            local_weight : local weight of the current client after training\n",
    "        \"\"\"\n",
    "        global_weight = self.request_global_weight()\n",
    "        if global_weight != None:\n",
    "            global_weight = np.array(global_weight)\n",
    "            self.model.set_weights(global_weight)\n",
    "        \n",
    "        self.model.fit(self.split_train_images, self.split_train_labels, epochs=10, batch_size=16, verbose=0)\n",
    "        N = self.request_total_num_data()\n",
    "        \n",
    "        local_weight = np.multiply(self.model.get_weights(), (self.local_data_num/N))\n",
    "        return local_weight\n",
    "    \n",
    "    def task(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            None : Delayed execution of Federated Learning task\n",
    "                  1. Check the client's current round\n",
    "                      1.1. If the current round is \n",
    "        \"\"\"\n",
    "        \n",
    "        #this is for executing on multiple devices\n",
    "        self.global_round = self.request_global_round()\n",
    "\n",
    "        if self.current_round >= self.max_round:\n",
    "            print(f\"Client {self.fed_id} finished\")\n",
    "            return \n",
    "\n",
    "        if self.global_round == self.current_round: #need update \n",
    "            global_weight = self.request_global_weight()\n",
    "\n",
    "            local_weight = self.train_local_model()\n",
    "\n",
    "            acc = self.validation(local_weight)\n",
    "\n",
    "            self.upload_local_weight(local_weight)\n",
    "\n",
    "            self.current_round += 1\n",
    "\n",
    "            time.sleep(self.time_delay)\n",
    "\n",
    "            return self.task()\n",
    "\n",
    "        else: #need to wait until other clients finish\n",
    "            time.sleep(self.time_delay * 2)\n",
    "            return self.task()\n",
    "\n",
    "        '''#this is for executing on multiple devices\n",
    "        else:\n",
    "            #this is for executing on one device\n",
    "            self.global_round = self.request_global_round()\n",
    "            \n",
    "\n",
    "\n",
    "            if self.global_round == self.current_round: #need update \n",
    "                start = time.time()\n",
    "                if not self.suppress:\n",
    "                    print(\"Request global weight...\")\n",
    "                global_weight = self.request_global_weight()\n",
    "                if not self.suppress:\n",
    "                    print(\"Global weight request done\")\n",
    "\n",
    "                if not self.suppress:\n",
    "                    print(\"Training local model...\")\n",
    "                local_weight = self.train_local_model()\n",
    "                if not self.suppress:\n",
    "                    print(\"Training done\")\n",
    "\n",
    "                acc = self.validation(local_weight)\n",
    "\n",
    "\n",
    "                if not self.suppress:\n",
    "                    print(\"Uploading local weight...\")\n",
    "                self.upload_local_weight(local_weight)\n",
    "                if not self.suppress:\n",
    "                    print(\"Weight upload done\")\n",
    "\n",
    "                if not self.suppress:\n",
    "                    print(\"=========================\")\n",
    "                end = time.time()\n",
    "\n",
    "                self.current_round += 1\n",
    "\n",
    "                threading.Timer(self.time_delay, self.task, [multiple_devices]).start()\n",
    "\n",
    "            else: #need to wait until other clients finish\n",
    "                threading.Timer(self.time_delay*2, self.task, [multiple_devices]).start()\n",
    "        #this is for executing on one device'''\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Usage --ip {ip} --p {port} --max {max round} --delay {time delay} --num {num samples}\")\n",
    "    parser.add_argument(\"--ip\", type=str, help=\"base ip address\", default=\"127.0.0.1\")\n",
    "    parser.add_argument(\"--max\", type=int, help=\"max round\", default=5)\n",
    "    parser.add_argument(\"--delay\", type=int, help=\"time delay\", default=5)\n",
    "    parser.add_argument(\"--num\", type=int, help=\"num samples\", default=600)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    IP = args.ip\n",
    "    max_round = args.max\n",
    "    time_delay = args.delay\n",
    "    num_samples = args.num\n",
    "    \n",
    "    client = Client(max_round = max_round, time_delay = time_delay, num_samples)\n",
    "    client.task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paramiko\n",
      "  Downloading paramiko-2.7.2-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[K     |████████████████████████████████| 206 kB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bcrypt>=3.1.3\n",
      "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.5 in /opt/conda/lib/python3.7/site-packages (from paramiko) (2.9.2)\n",
      "Collecting pynacl>=1.0.1\n",
      "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
      "\u001b[K     |████████████████████████████████| 961 kB 24.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from bcrypt>=3.1.3->paramiko) (1.14.0)\n",
      "Requirement already satisfied: cffi>=1.1 in /opt/conda/lib/python3.7/site-packages (from bcrypt>=3.1.3->paramiko) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko) (2.20)\n",
      "Installing collected packages: bcrypt, pynacl, paramiko\n",
      "Successfully installed bcrypt-3.2.0 paramiko-2.7.2 pynacl-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ping3\n",
      "  Downloading ping3-3.0.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: ping3\n",
      "Successfully installed ping3-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ping3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jetson_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jetson_server.py\n",
    "import paramiko\n",
    "import getpass\n",
    "from ping3 import ping\n",
    "import time\n",
    "\n",
    "###### JETSON NANO ADDRESS ######\n",
    "JETSON_IP = \"147.47.200.209\"\n",
    "MIN_PORT = \"20101\"\n",
    "MAX_PORT = \"20136\"\n",
    "EXPERIMENT = 1\n",
    "MAX_ROUND = 5\n",
    "TIME_DELAY = 5\n",
    "USERNAME = \"jetson\"\n",
    "PASSWORD = \"jetson\"\n",
    "\n",
    "###### SERVER ADDRESS ######\n",
    "SERVER_IP = \"147.47.200.178:9103\"\n",
    "\n",
    "# (Resets the server and change the total number of clients recorded in the server)\n",
    "def initialize_server(client_number):\n",
    "    base_url = f\"http://{IP}:{PORT}/\"\n",
    "    client_url = base_url + \"client_num\"\n",
    "    assert(isinstance(client_number, int))\n",
    "\n",
    "    reset_url = base_url + \"reset\"\n",
    "    reset_result = requests.get(reset_url)\n",
    "    \n",
    "    max_round_to_json = json.dumps(client_number)\n",
    "    client_result = requests.put(client_url, data=max_round_to_json)\n",
    "    \n",
    "    assert reset_result.text == \"Request OK\" and client_result.text == \"Request PUT OK\", \"Server Init Failed\"\n",
    "    print(\"Server reset success\" if reset_result.text == \"Request OK\" else \"Server reset failed\")\n",
    "    \n",
    "max_round = 3 # Any positive integer (not tested for extremely large value)\n",
    "experiment = 1 # 1,2,3,4\n",
    "time_delay = 5 # time in seconds to wait until retry (not tested for extremely large value)\n",
    "suppress = False # 모든 출력을 보고싶지 않으면 True\n",
    "initialize_server()\n",
    "\n",
    "#Instantiate the clients and create Threads to execute them\n",
    "#client.task is recursively called within until all clients finish training (including itself)\n",
    "\n",
    "\"\"\"\n",
    "Initialize multiple Jetson Nanos and start FL\n",
    "\"\"\"\n",
    "class Jetson:\n",
    "    def __init__(self, IP='147.47.200.22', min_port = \"20101\", max_port=\"20202\"):\n",
    "        assert int(min_port) < int(max_port), \"max port must be >= min port\"\n",
    "        self.addresses = [IP+f\":{i}\" for i in range(int(min_port), int(max_port)+1)]\n",
    "        self.available = [] #available addresses\n",
    "        \n",
    "        self.test_jetson_nano()\n",
    "        \n",
    "    def __ping(self, address):\n",
    "        resp = ping(address)\n",
    "        \n",
    "        if resp == False:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def __initialize(self, address, experiment, max_round, time_delay, num_samples):\n",
    "        # SSH 로 접속 해서 Client들 initialize 시키고 필요한 것들 하기\n",
    "        cli = paramiko.SSHClient()\n",
    "        cli.set_missing_host_key_policy(paramiko.AutoAddPolicy)\n",
    "\n",
    "        server = address  # 호스트명이나 IP 주소\n",
    "        user = USERNAME\n",
    "        pwd = PASSWORD\n",
    "\n",
    "        cli.connect(server, port=22, username=user, password=pwd)\n",
    "        stdin, stdout, stderr = cli.exec_command(f\"python jetson_client.py --ip {SERVER_IP} --max {max_round} --delay {time_delay} --num {num_samples}\")\n",
    "        \n",
    "        lines = stdout.readlines()\n",
    "        print(''.join(lines))\n",
    "        cli.close()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def start_federated_learning(self):\n",
    "        \n",
    "    def test_jetson_nano(self):\n",
    "        for address in self.addresses:\n",
    "            temp = self.__ping(address)    \n",
    "            if temp == True:\n",
    "                self.available.append(address)\n",
    "                \n",
    "    def initialize_jetson_nano(self, experiment, max_round, time_delay, num_samples):\n",
    "        for address in self.available:\n",
    "            self.__initialize(address, experiment, max_round, time_delay, num_samples)\n",
    "    \n",
    "    def fetch_results(self):\n",
    "        return FederatedServer.accuracies\n",
    "    \n",
    "    \n",
    "start = time.time()\n",
    "jetson = Jetson(IP=JETSON_IP, min_port = MIN_PORT, max_port=MAX_PORT)\n",
    "\n",
    "jetson.initialize_jetson_nano(experiment=experiment, max_round=max_round, time_delay=time_delay, suppress=suppress)\n",
    "jetson.start_federated_learning()\n",
    "\n",
    "results = jetson.fetch_results()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Federated learning took {end-start} seconds\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rounds = max(results[0].keys())\n",
    "ids = max(results.keys())\n",
    "result = np.zeros((ids, rounds))\n",
    "\n",
    "for round_ in rounds:\n",
    "    for cur_id in range(ids):\n",
    "        result[cur_id, round_] = results[round_][cur_id]\n",
    "        \n",
    "def get_result(id, round):\n",
    "    return result[id, round]\n",
    "\n",
    "def plot_accuracy():\n",
    "    rounds = list(range(len(result[0])))\n",
    "    rounds = rounds.astype(\"int8\")\n",
    "    for fed_id in range(len(result)):\n",
    "        accs = result[fed_id, :]\n",
    "        plt.plot(rounds, accs, label=f\"{fed_id}\")\n",
    "        plt.xticks(range(len(result[0]+1)))\n",
    "    plt.title(\"Accuracies of the clients (%)\")\n",
    "    plt.xlabel(\"rounds\")\n",
    "    plt.ylabel(\"accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "    \n",
    "plot_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW {#}\n",
    "* 실습에서 컴퓨터 하나로 Federated learning을 진행해 보았다.\n",
    "* 이번 과제는 미리 설치해 놓은 여러 대의 Jetson Nano를 사용해서 실제로 네트워크 상에서 Federated learning을 진행하는 것이다. \n",
    "* 4개의 실험 (experiment:1,2,3,4)을 진행해보고, 결과를 보고서로 작성하여 제출.\n",
    "* 보고서에는 실습에서의 결과와의 비교가 포함되어야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
