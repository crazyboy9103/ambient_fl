{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ef68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from random import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Quiet tensorflow error messages\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder): # inherits JSONEncoder \n",
    "    def default(self, o):\n",
    "        if isinstance(o, np.ndarray):\n",
    "            return o.tolist()\n",
    "        return json.JSONEncoder.default(self, o)\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, max_round: int, time_delay = 5, suppress=True, num_samples=600, client_id = 0, experiment = 1):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            experiment : Desired data split type (1~4)\n",
    "            max_round : the maximum number of rounds that should be trained (arbitrary integer)\n",
    "            model : the NN model type (either 'ann' or 'cnn')\n",
    "            time_delay : the time delay until the next local check (arbitrary positive integer) \n",
    "                        (Need to increase this value if one round of training takes much longer than current time_delay. \n",
    "                        The reason is that any network communication until next round after the client has already uploaded \n",
    "                        the parameters for current round increases network overhead. Thus, higher time_delay will make communication\n",
    "                        more stable while increasing the absolute time it takes. Requires careful selection of this value.)\n",
    "            suppress : boolean value to print the logs\n",
    "        \n",
    "        @return: \n",
    "            None : Initializes the variables\n",
    "                   Setup the urls for communication\n",
    "                   Fetch client's id from the server\n",
    "                   Downloads MNIST dataset and splits\n",
    "                   Build model\n",
    "        \"\"\"\n",
    "        \n",
    "        '''\n",
    "        Urls\n",
    "        '''\n",
    "        \n",
    "        self.base_url = \"http://127.0.0.1:9103/\" # Base Url\n",
    "        self.put_weight_url =  self.base_url + f\"put_weight/{client_id}\"\n",
    "        self.get_weight_url =  self.base_url + \"get_server_weight\" # Url that we send or fetch weight parameters\n",
    "        self.round_url =  self.base_url + \"get_server_round\" \n",
    "\n",
    "        '''\n",
    "        Initial setup\n",
    "        '''\n",
    "        self.experiment = experiment\n",
    "        self.client_id = client_id\n",
    "        self.time_delay = time_delay\n",
    "        self.suppress = suppress\n",
    "        self.global_round = self.request_global_round()\n",
    "        self.current_round = 0\n",
    "        self.max_round = max_round # Set the maximum number of rounds\n",
    "        \n",
    "        '''\n",
    "        Downloads MNIST dataset and prepares (train_x, train_y), (test_x, test_y)\n",
    "        '''\n",
    "        self.train_images, self.train_labels = None, None\n",
    "        self.test_images, self.test_labels = None, None\n",
    "        self.prepare_images()\n",
    "        \n",
    "        self.train_index_list = None\n",
    "        self.test_index_list = None\n",
    "        self.split_train_images = []\n",
    "        self.split_train_labels = []\n",
    "        \n",
    "        self.local_data_num = 0\n",
    "        \n",
    "        '''\n",
    "        Builds model\n",
    "        '''\n",
    "        self.model = None\n",
    "        self.build_cnn_model()\n",
    "        \n",
    "    def prepare_images(self):\n",
    "        \"\"\"\n",
    "        return: \n",
    "            None : Prepares MNIST images in the required format for each model\n",
    "            \n",
    "        \"\"\"\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_images, self.train_labels), (self.test_images, self.test_labels) = mnist.load_data()\n",
    "        self.train_images, self.test_images = self.train_images / 255, self.test_images / 255\n",
    "        \n",
    "        # For CNN, add dummy channel to feed the images to CNN\n",
    "        self.train_images=self.train_images.reshape(-1,28, 28, 1)\n",
    "        self.test_images=self.test_images.reshape(-1,28, 28, 1)\n",
    "            \n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            None : saves the CNN model in self.model variable \n",
    "        \"\"\"\n",
    "        #This model definition must be same in the server (Federated.py)\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "    def data_split(self, num_samples):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            num_samples : The number of sample images in each client. This value is used for equally\n",
    "                          sized dataset\n",
    "        \n",
    "        @return: \n",
    "            None : Split the dataset depending on the self.experiment value\n",
    "           \n",
    "                If self.experiment is 1: Uniform data split: We take equal amount of data from each class (iid)\n",
    "                If self.experiment is 2: Random data split1: We take equal amount of data, but not uniformly distributed across classes\n",
    "                If self.experiment is 3: Random data split2: We take different amount of data and not uniformly distributed across classes\n",
    "                If self.experiment is 4: Skewed: We take disproportionate amount of data for some classes\n",
    "                        \n",
    "        \"\"\"\n",
    "        if self.train_index_list is None or self.test_index_list is None:\n",
    "            self.train_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "            self.test_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "            for i, v in enumerate(self.train_labels):\n",
    "                self.train_index_list[v].append(i)\n",
    "\n",
    "            for i, v in enumerate(self.test_labels):\n",
    "                self.test_index_list[v].append(i)\n",
    "\n",
    "        \n",
    "        self.split_train_images = []\n",
    "        self.split_train_labels = []\n",
    "        \n",
    "        if self.experiment == 1: #uniform data split\n",
    "            self.local_data_num = num_samples\n",
    "            \n",
    "            for i in range(len(self.train_index_list)):\n",
    "                indices = self.train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=num_samples//10)\n",
    "                \n",
    "                self.split_train_images.extend(self.train_images[random_indices])\n",
    "                self.split_train_labels.extend(self.train_labels[random_indices])\n",
    "            \n",
    "\n",
    "        elif self.experiment == 2: # Randomly selected, equally sized dataset\n",
    "            self.local_data_num = num_samples\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=num_samples)\n",
    "            self.split_train_images = self.train_images[random_indices]\n",
    "            self.split_train_labels = self.train_labels[random_indices]\n",
    "\n",
    "            counts = [0 for _ in range(10)]\n",
    "            \n",
    "            for label in self.train_labels[random_indices]:\n",
    "                counts[label] += 1\n",
    "            \n",
    "        elif self.experiment == 3: # Randomly selected, differently sized dataset\n",
    "            n = np.random.randint(1, num_samples)\n",
    "            self.local_data_num = n\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=n)\n",
    "            self.split_train_images = self.train_images[random_indices]\n",
    "            self.split_train_labels = self.train_labels[random_indices]\n",
    "            \n",
    "\n",
    "            counts = [0 for _ in range(10)]\n",
    "            \n",
    "            for label in self.train_labels[random_indices]:\n",
    "                counts[label] += 1\n",
    "  \n",
    "        elif self.experiment == 4: #Skewed\n",
    "            temp = [i for i in range(10)]\n",
    "            skewed_numbers = np.random.choice(temp, np.random.randint(1, 10))\n",
    "            non_skewed_numbers = list(set(temp)-set(skewed_numbers))\n",
    "            N = 0\n",
    "            \n",
    "            counts = [0 for _ in range(10)]\n",
    "            \n",
    "            for i in skewed_numbers:\n",
    "                n = np.random.randint(50, 60)\n",
    "                N += n\n",
    "                \n",
    "                indices = self.train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                self.split_train_images.extend(self.train_images[random_indices])\n",
    "                self.split_train_labels.extend(self.train_labels[random_indices])\n",
    "                \n",
    "                counts[i] += n\n",
    "            \n",
    "                \n",
    "            for i in non_skewed_numbers:\n",
    "                n = np.random.randint(1, 10)\n",
    "                N += n\n",
    "                \n",
    "                indices = self.train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                self.split_train_images.extend(self.train_images[random_indices])\n",
    "                self.split_train_labels.extend(self.train_labels[random_indices])\n",
    "                \n",
    "                counts[i] += n\n",
    "            \n",
    "            self.local_data_num = N\n",
    "        \n",
    "        self.split_train_images = np.array(self.split_train_images)\n",
    "        self.split_train_labels = np.array(self.split_train_labels)\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "    def update_total_num_data(self, num_data):\n",
    "        \"\"\"\n",
    "        num_data : the number of training images that the current client has\n",
    "        \n",
    "        update the total number of training images that is stored in the server\n",
    "        \"\"\"\n",
    "        update_num_data_url =  self.base_url + f\"update_num_data/{self.client_id}/{num_data}\"\n",
    "        requests.get(update_num_data_url)\n",
    "        \n",
    "\n",
    "    \n",
    "    def request_global_round(self):\n",
    "        \"\"\"\n",
    "        result : Current global round that the server is in\n",
    "        \"\"\"\n",
    "        result = requests.get(self.round_url)\n",
    "        result = result.json()\n",
    "        return result\n",
    "    \n",
    "    def request_global_weight(self):\n",
    "        \"\"\"\n",
    "        global_weight : Up-to-date version of the model parameters\n",
    "        \"\"\"\n",
    "        result = requests.get(self.get_weight_url)\n",
    "        result_data = result.json()\n",
    "        \n",
    "        global_weight = None\n",
    "        if result_data is not None:\n",
    "            global_weight = []\n",
    "            for i in range(len(result_data)):\n",
    "                temp = np.array(result_data[i], dtype=np.float32)\n",
    "                global_weight.append(temp)\n",
    "            \n",
    "        return global_weight\n",
    "\n",
    "    def upload_local_weight(self, local_weight):\n",
    "        \"\"\"\n",
    "        local_weight : the local weight that current client has converged to\n",
    "        \n",
    "        Add current client's weights to the server (Server accumulates these from multiple clients and computes the global weight)\n",
    "        \"\"\"\n",
    "        local_weight_to_json = json.dumps(local_weight, cls=NumpyEncoder)\n",
    "        requests.put(self.put_weight_url, data=local_weight_to_json)\n",
    "        \n",
    "    def train_local_model(self):\n",
    "        \"\"\"\n",
    "        local_weight : local weight of the current client after training\n",
    "        \"\"\"\n",
    "        global_weight = self.request_global_weight()\n",
    "        if global_weight != None:\n",
    "            global_weight = np.array(global_weight)\n",
    "            self.model.set_weights(global_weight)\n",
    "            \n",
    "        \n",
    "        self.model.fit(self.split_train_images, self.split_train_labels, epochs=10, batch_size=8, verbose=0)\n",
    "        local_weight = self.model.get_weights()\n",
    "        return local_weight\n",
    "    \n",
    "    def task(self):\n",
    "        \"\"\"\n",
    "        Federated learning task\n",
    "        1. If the current round is larger than the max round that we set, finish\n",
    "        2. If the global round = current client's round, the client needs update\n",
    "        3. Otherwise, we need to wait until other clients to finish\n",
    "        \"\"\"\n",
    "        \n",
    "        #this is for executing on multiple devices\n",
    "        self.global_round = self.request_global_round()\n",
    "        \n",
    "        print(\"global round\", self.global_round)\n",
    "        print(\"current round\", self.current_round)\n",
    "        if self.current_round >= self.max_round:\n",
    "            print(f\"Client {self.client_id} finished\")\n",
    "            return \n",
    "\n",
    "        if self.global_round == self.current_round: #need update \n",
    "            self.data_split(num_samples=self.local_data_num)\n",
    "            self.update_total_num_data(self.local_data_num) \n",
    "            local_weight = self.train_local_model()\n",
    "            self.upload_local_weight(local_weight)\n",
    "            self.current_round += 1\n",
    "            time.sleep(self.time_delay)\n",
    "            return self.task()\n",
    "\n",
    "        else: #need to wait until other clients finish\n",
    "            time.sleep(self.time_delay * 2)\n",
    "            return self.task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d8af5",
   "metadata": {},
   "source": [
    "# Fed learning on local computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd25efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_NUM = 5\n",
    "EXPERIMENT = 1\n",
    "MAX_ROUND = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe58fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "init = requests.get(f\"http://localhost:9103/initialize/{CLIENT_NUM}/{EXPERIMENT}/{MAX_ROUND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1384bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initialized server'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461cb30b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global round 0\n",
      "current round 0\n",
      "global round 0\n",
      "current round 0\n",
      "global round 0\n",
      "current round 0\n",
      "global round 0\n",
      "current round 0\n",
      "global round 0\n",
      "current round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "        self.run()\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 870, in run\n",
      "self.run()\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 870, in run\n",
      "        self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 303, in task\n",
      "self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 303, in task\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 279, in train_local_model\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 279, in train_local_model\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1104, in fit\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1104, in fit\n",
      "    epoch_logs = copy.copy(logs)\n",
      "UnboundLocalError: local variable 'logs' referenced before assignment\n",
      "Exception in thread Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    epoch_logs = copy.copy(logs)\n",
      "UnboundLocalError: local variable 'logs' referenced before assignment\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 870, in run\n",
      "        self.run()\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 870, in run\n",
      "self.run()\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\threading.py\", line 870, in run\n",
      "        self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 303, in task\n",
      "self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 303, in task\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 303, in task\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 279, in train_local_model\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 279, in train_local_model\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\", line 279, in train_local_model\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1104, in fit\n",
      "      File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\n",
      "return method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1104, in fit\n",
      "        return method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\david\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1104, in fit\n",
      "epoch_logs = copy.copy(logs)\n",
      "UnboundLocalError: local variable 'logs' referenced before assignment\n",
      "    epoch_logs = copy.copy(logs)\n",
      "UnboundLocalError: local variable 'logs' referenced before assignment\n",
      "    epoch_logs = copy.copy(logs)\n",
      "UnboundLocalError: local variable 'logs' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "clients = []\n",
    "\n",
    "\n",
    "for i in range(CLIENT_NUM):\n",
    "    client = Client(max_round =MAX_ROUND, \n",
    "                    time_delay = 5, \n",
    "                    num_samples = 100,  \n",
    "                    suppress=True, \n",
    "                    client_id = i, \n",
    "                    experiment = EXPERIMENT)\n",
    "    clients.append(client) #retain references to the clients\n",
    "\n",
    "threads = []\n",
    "\n",
    "for client in clients:\n",
    "    t = threading.Thread(target=client.task)\n",
    "    threads.append(t)\n",
    "    \n",
    "for t in threads:\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697fc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = requests.get(\"http://localhost:9103/initialize/1/1/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcafe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(max_round =5 , \n",
    "                    time_delay = 5, \n",
    "                    num_samples = 100,  \n",
    "                    suppress=True, \n",
    "                    client_id = 0, \n",
    "                    experiment = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e661932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global round 0\n",
      "current round 0\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16968/3787327017.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\u001b[0m in \u001b[0;36mtask\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_data_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_total_num_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_data_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[0mlocal_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_local_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload_local_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_round\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16968/2339583023.py\u001b[0m in \u001b[0;36mtrain_local_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_train_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_train_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[0mlocal_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlocal_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\test_fl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m         \u001b[1;31m# Run validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'logs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "client.task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21ea8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
