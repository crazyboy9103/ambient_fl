{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Federated learning이란 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Django API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Django를 사용해서 Federated Learning을 위한 서버를 구축할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached https://files.pythonhosted.org/packages/3a/5f/47e578b3ae79e2624e205445ab77a1848acdaa2929a00eeef6b16eaaeb20/numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.16.6\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy (from -r FL_Client/requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/3a/5f/47e578b3ae79e2624e205445ab77a1848acdaa2929a00eeef6b16eaaeb20/numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting requests (from -r FL_Client/requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/92/96/144f70b972a9c0eabbd4391ef93ccd49d0f2747f4f6a2a2738e99e5adc65/requests-2.26.0-py2.py3-none-any.whl\n",
      "Collecting scipy (from -r FL_Client/requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/24/40/11b12af7f322c1e20446c037c47344d89bab4922b8859419d82cf56d796d/scipy-1.2.3-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting six (from -r FL_Client/requirements.txt (line 4))\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Collecting tensorflow-estimator (from -r FL_Client/requirements.txt (line 5))\n",
      "  Using cached https://files.pythonhosted.org/packages/db/de/3a71ad41b87f9dd424e3aec3b0794a60f169fa7e9a9a1e3dd44290b86dd6/tensorflow_estimator-2.7.0-py2.py3-none-any.whl\n",
      "Collecting tensorflow-gpu (from -r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/41/6d/2348df00a34baaabdef0fdb4f46f962f7a8a6720362c26c3a44a249767ea/tensorflow_gpu-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting urllib3 (from -r FL_Client/requirements.txt (line 7))\n",
      "  Using cached https://files.pythonhosted.org/packages/af/f4/524415c0744552cce7d8bf3669af78e8a069514405ea4fcbd0cc44733744/urllib3-1.26.7-py2.py3-none-any.whl\n",
      "Collecting Keras (from -r FL_Client/requirements.txt (line 8))\n",
      "  Using cached https://files.pythonhosted.org/packages/6b/8b/065f94ba03282fa41b2d76942b87a180a9913312c4611ea7d6508fbbc114/keras-2.7.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn (from -r FL_Client/requirements.txt (line 9))\n",
      "  Using cached https://files.pythonhosted.org/packages/31/9f/042db462417451e81035c3d43b722e88450c628a33dfda69777a801b0d40/scikit_learn-0.20.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting tensorflow (from -r FL_Client/requirements.txt (line 10))\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/59/d88fe8c58ffb66aca21d03c0e290cd68327cc133591130c674985e98a482/tensorflow-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting paramiko (from -r FL_Client/requirements.txt (line 11))\n",
      "  Using cached https://files.pythonhosted.org/packages/72/b5/7b99a3da446338c8b5c73da549e8bc8d8c3a066a9d535e64191ac3b77137/paramiko-2.8.0-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5; python_version < \"3\" (from requests->-r FL_Client/requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests->-r FL_Client/requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/37/45/946c02767aabb873146011e665728b680884cd8fe70dde973c640e45b775/certifi-2021.10.8-py2.py3-none-any.whl\n",
      "Collecting chardet<5,>=3.0.2; python_version < \"3\" (from requests->-r FL_Client/requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "Collecting mock>=2.0.0 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "Collecting wrapt>=1.11.1 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/3b/e8/702ff292e05c8d74c6babde2266f6f62acc0d851784081c6aca5dff8a455/wrapt-1.13.3-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/74/4e/9f3cb458266ef5cdeaa1e72a90b9eda100e3d1803cbd7ec02f0846da83c3/protobuf-3.18.0-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "Collecting wheel (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/04/80/cad93b40262f5d09f6de82adbee452fd43cdff60830b56a74c5930f7e277/wheel-0.37.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/37/e6a7af1c92c5b68fb427f853b06164b56ea92126bcfd87784334ec5e4d42/tensorboard-1.14.0-py2-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "Collecting absl-py>=0.7.0 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "Collecting enum34>=1.1.6 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/6f/2c/a9386903ece2ea85e9807e0e062174dc26fdce8b05f216d00491be29fad5/enum34-1.1.10-py2-none-any.whl\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/59/22/38238cd9b83dd8a857abd7c907b8fe68ceff1611ab3ca5f0e80a5e025956/google_pasta-0.2.0-py2-none-any.whl\n",
      "Collecting backports.weakref>=1.0rc1 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting pynacl>=1.0.1 (from paramiko->-r FL_Client/requirements.txt (line 11))\n",
      "  Using cached https://files.pythonhosted.org/packages/de/63/bb36279da38df643c6df3a8a389f29a6ff4a8854468f4c9b9d925b27d57d/PyNaCl-1.4.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting bcrypt>=3.1.3 (from paramiko->-r FL_Client/requirements.txt (line 11))\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/36/9a0227d048e98409f012570f7bef8a8c2373b9c9c5dfbf82963cbae05ede/bcrypt-3.1.7-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting cryptography>=2.5 (from paramiko->-r FL_Client/requirements.txt (line 11))\n",
      "  Using cached https://files.pythonhosted.org/packages/86/41/44173175d378c9e3a73294bb33c73725f7726def5ce267af6bd11b72eb23/cryptography-3.3.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting futures>=2.2.0; python_version < \"3.2\" (from grpcio>=1.8.6->tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/d8/a6/f46ae3f1da0cd4361c344888f59ec2f5785e69c872e175a748ef6071cdb5/futures-3.3.0-py2-none-any.whl\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting h5py (from keras-applications>=1.0.6->tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/12/90/3216b8f6d69905a320352a9ca6802a8e39fdb1cd93133c3d4163db8d5f19/h5py-2.10.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting setuptools>=41.0.0 (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/b7/182161210a13158cd3ccc41ee19aadef54496b74f2817cc147006ec932b4/setuptools-44.1.1-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu->-r FL_Client/requirements.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Collecting cffi>=1.4.1 (from pynacl>=1.0.1->paramiko->-r FL_Client/requirements.txt (line 11))\n",
      "  Using cached https://files.pythonhosted.org/packages/26/28/fb01ff898aa7c93e4774799b3dc0a3693cfee48c5ea4e524ce30e6b10e7e/cffi-1.15.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting ipaddress; python_version < \"3\" (from cryptography>=2.5->paramiko->-r FL_Client/requirements.txt (line 11))\n",
      "  Using cached https://files.pythonhosted.org/packages/c2/f8/49697181b1651d8347d24c095ce46c7346c37335ddc7d255833e7cde674d/ipaddress-1.0.23-py2.py3-none-any.whl\n",
      "Collecting pycparser (from cffi>=1.4.1->pynacl>=1.0.1->paramiko->-r FL_Client/requirements.txt (line 11))\n",
      "  Using cached https://files.pythonhosted.org/packages/62/d5/5f610ebe421e85889f2e55e33b7f9a6795bd982198517d912eb1c76e1a53/pycparser-2.21-py2.py3-none-any.whl\n",
      "Installing collected packages: numpy, idna, certifi, urllib3, chardet, requests, scipy, six, tensorflow-estimator, enum34, futures, grpcio, funcsigs, mock, h5py, keras-applications, wrapt, protobuf, keras-preprocessing, gast, wheel, setuptools, werkzeug, absl-py, markdown, tensorboard, termcolor, google-pasta, backports.weakref, astor, tensorflow-gpu, Keras, scikit-learn, tensorflow, pycparser, cffi, pynacl, bcrypt, ipaddress, cryptography, paramiko\n"
     ]
    }
   ],
   "source": [
    "!pip install -r FL_Client/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아래는 학습을 위한 ```Client``` 클래스이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa716c83f894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "from random import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Quiet tensorflow error messages\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder): # inherits JSONEncoder \n",
    "    def default(self, o):\n",
    "        if isinstance(o, np.ndarray):\n",
    "            return o.tolist()\n",
    "        return json.JSONEncoder.default(self, o)\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, max_round: int, time_delay = 5, suppress=True, num_samples=600, client_id = 0, experiment = 1):\n",
    "        '''\n",
    "        Urls\n",
    "        '''\n",
    "        self.base_url = \"http://127.0.0.1:9103/\" # Base Url\n",
    "        self.put_weight_url =  self.base_url + \"put_weight/\" + str(client_id)\n",
    "        self.get_weight_url =  self.base_url + \"get_server_weight\" # Url that we send or fetch weight parameters\n",
    "        self.round_url =  self.base_url + \"get_server_round\" \n",
    "\n",
    "        '''\n",
    "        Initial setup\n",
    "        '''\n",
    "        self.experiment = experiment\n",
    "        self.client_id = client_id\n",
    "        self.time_delay = time_delay\n",
    "        self.suppress = suppress\n",
    "        self.global_round = self.request_global_round()\n",
    "        self.current_round = 0\n",
    "        self.max_round = max_round # Set the maximum number of rounds\n",
    "        \n",
    "        '''\n",
    "        Downloads MNIST dataset and prepares (train_x, train_y), (test_x, test_y)\n",
    "        '''\n",
    "        self.train_images, self.train_labels, self.test_images, self.test_labels = self.prepare_images()\n",
    "        self.split_train_images, self.split_train_labels = self.data_split(num_samples)\n",
    "        self.local_data_num = len(self.split_train_labels)\n",
    "        \n",
    "        '''\n",
    "        Builds model\n",
    "        '''\n",
    "        self.model = self.build_cnn_model()\n",
    "        \n",
    "    def prepare_images(self):\n",
    "        \"\"\"\n",
    "        return: \n",
    "            None : Prepares MNIST images in the required format for each model\n",
    "            \n",
    "        \"\"\"\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "        train_images, test_images = train_images / 255, test_images / 255\n",
    "        \n",
    "        # For CNN, add dummy channel to feed the images to CNN\n",
    "        train_images=train_images.reshape(-1,28, 28, 1)\n",
    "        test_images=test_images.reshape(-1,28, 28, 1)\n",
    "        return train_images, train_labels, test_images, test_labels\n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            None : saves the CNN model in self.model variable \n",
    "        \"\"\"\n",
    "        #This model definition must be same in the server (Federated.py)\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "        \n",
    "    def data_split(self, num_samples):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            num_samples : The number of sample images in each client. This value is used for equally\n",
    "                          sized dataset\n",
    "        \n",
    "        @return: \n",
    "            None : Split the dataset depending on the self.experiment value\n",
    "           \n",
    "                If self.experiment is 1: Uniform data split: We take equal amount of data from each class (iid)\n",
    "                If self.experiment is 2: Random data split1: We take equal amount of data, but not uniformly distributed across classes\n",
    "                If self.experiment is 3: Random data split2: We take different amount of data and not uniformly distributed across classes\n",
    "                If self.experiment is 4: Skewed: We take disproportionate amount of data for some classes\n",
    "                        \n",
    "        \"\"\"\n",
    "        \n",
    "        train_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "        test_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "        for i, v in enumerate(self.train_labels):\n",
    "            train_index_list[v].append(i)\n",
    "\n",
    "        for i, v in enumerate(self.test_labels):\n",
    "            test_index_list[v].append(i)\n",
    "\n",
    "        \n",
    "        split_train_images = []\n",
    "        split_train_labels = []\n",
    "        \n",
    "        \"\"\"\n",
    "        Todo : split the data according to the instructions        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        For each experiment, you must\n",
    "            1. save the total number of samples to self.local_data_num variable\n",
    "            2. add the split images and labels into self.split_train_images and self.split_train_labels respectively\n",
    "        \"\"\"\n",
    "        if self.experiment == 1: #uniform data split\n",
    "            # all \n",
    "            self.local_data_num = num_samples\n",
    "            \n",
    "            for i in range(len(train_index_list)):\n",
    "                indices = train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=num_samples//10)\n",
    "                \n",
    "                split_train_images.extend(self.train_images[random_indices])\n",
    "                split_train_labels.extend(self.train_labels[random_indices])\n",
    "            \n",
    "\n",
    "        elif self.experiment == 2: # Randomly selected, equally sized dataset\n",
    "            self.local_data_num = num_samples\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=num_samples)\n",
    "            split_train_images = self.train_images[random_indices]\n",
    "            split_train_labels = self.train_labels[random_indices]\n",
    "\n",
    "        \n",
    "            \n",
    "        elif self.experiment == 3: # Randomly selected, differently sized dataset\n",
    "            n = np.random.randint(1, num_samples)\n",
    "            self.local_data_num = n\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=n)\n",
    "            split_train_images = self.train_images[random_indices]\n",
    "            split_train_labels = self.train_labels[random_indices]\n",
    "            \n",
    "     \n",
    "  \n",
    "        elif self.experiment == 4: #Skewed\n",
    "            temp = [i for i in range(10)]\n",
    "            skewed_numbers = np.random.choice(temp, np.random.randint(1, 10))\n",
    "            non_skewed_numbers = list(set(temp)-set(skewed_numbers))\n",
    "            N = 0\n",
    "          \n",
    "            for i in skewed_numbers:\n",
    "                n = np.random.randint(50, 60)\n",
    "                N += n\n",
    "                \n",
    "                indices = train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                split_train_images.extend(self.train_images[random_indices])\n",
    "                split_train_labels.extend(self.train_labels[random_indices])\n",
    "                \n",
    "                \n",
    "            for i in non_skewed_numbers:\n",
    "                n = np.random.randint(1, 10)\n",
    "                N += n\n",
    "                \n",
    "                indices = train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                split_train_images.extend(self.train_images[random_indices])\n",
    "                split_train_labels.extend(self.train_labels[random_indices])\n",
    "      \n",
    "            \n",
    "            self.local_data_num = N\n",
    "        \n",
    "        split_train_images = np.array(split_train_images)\n",
    "        split_train_labels = np.array(split_train_labels)\n",
    "        return split_train_images, split_train_labels\n",
    "\n",
    "        \n",
    "        \n",
    "    def update_total_num_data(self, num_data):\n",
    "        \"\"\"\n",
    "        num_data : the number of training images that the current client has\n",
    "        \n",
    "        update the total number of training images that is stored in the server\n",
    "        \"\"\"\n",
    "        update_num_data_url =  self.base_url + \"update_num_data/\"+str(self.client_id)+\"/{num_data}\"\n",
    "        requests.get(update_num_data_url)\n",
    "        \n",
    "\n",
    "    \n",
    "    def request_global_round(self):\n",
    "        \"\"\"\n",
    "        result : Current global round that the server is in\n",
    "        \"\"\"\n",
    "        result = requests.get(self.round_url)\n",
    "        result = result.json()\n",
    "        return result\n",
    "    \n",
    "    def request_global_weight(self):\n",
    "        \"\"\"\n",
    "        global_weight : Up-to-date version of the model parameters\n",
    "        \"\"\"\n",
    "        result = requests.get(self.get_weight_url)\n",
    "        result_data = result.json()\n",
    "        \n",
    "        global_weight = None\n",
    "        if result_data is not None:\n",
    "            global_weight = []\n",
    "            for i in range(len(result_data)):\n",
    "                temp = np.array(result_data[i], dtype=np.float32)\n",
    "                global_weight.append(temp)\n",
    "            \n",
    "        return global_weight\n",
    "\n",
    "    def upload_local_weight(self, local_weight):\n",
    "        \"\"\"\n",
    "        local_weight : the local weight that current client has converged to\n",
    "        \n",
    "        Add current client's weights to the server (Server accumulates these from multiple clients and computes the global weight)\n",
    "        \"\"\"\n",
    "        local_weight_to_json = json.dumps(local_weight, cls=NumpyEncoder)\n",
    "        requests.put(self.put_weight_url, data=local_weight_to_json)\n",
    "        \n",
    "    def train_local_model(self):\n",
    "        \"\"\"\n",
    "        local_weight : local weight of the current client after training\n",
    "        \"\"\"\n",
    "        global_weight = self.request_global_weight()\n",
    "        if global_weight != None:\n",
    "            global_weight = np.array(global_weight)\n",
    "            self.model.set_weights(global_weight)\n",
    "            \n",
    "        \n",
    "        self.model.fit(self.split_train_images, self.split_train_labels, epochs=10, batch_size=8, verbose=0)\n",
    "        local_weight = self.model.get_weights()\n",
    "        return local_weight\n",
    "    \n",
    "    def task(self):\n",
    "        \"\"\"\n",
    "        Federated learning task\n",
    "        1. If the current round is larger than the max round that we set, finish\n",
    "        2. If the global round = current client's round, the client needs update\n",
    "        3. Otherwise, we need to wait until other clients to finish\n",
    "        \"\"\"\n",
    "        \n",
    "        #this is for executing on multiple devices\n",
    "        self.global_round = self.request_global_round()\n",
    "        \n",
    "        print(\"global round\", self.global_round)\n",
    "        print(\"current round\", self.current_round)\n",
    "        if self.current_round >= self.max_round:\n",
    "            print(f\"Client {self.client_id} finished\")\n",
    "            return \n",
    "\n",
    "        if self.global_round == self.current_round: #need update \n",
    "            self.data_split(num_samples=self.local_data_num)\n",
    "            self.update_total_num_data(self.local_data_num) \n",
    "            local_weight = self.train_local_model()\n",
    "            self.upload_local_weight(local_weight)\n",
    "            self.current_round += 1\n",
    "            time.sleep(self.time_delay)\n",
    "            return self.task()\n",
    "\n",
    "        else: #need to wait until other clients finish\n",
    "            time.sleep(self.time_delay * 2)\n",
    "            return self.task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile FL_Server/app/Federated.py\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from app import numpy_encoder\n",
    "import os \n",
    "    \n",
    "class FederatedServer:\n",
    "    client_number = 5 # 전체 클라이언트 개수\n",
    "    server_weight = None # 현재 서버에 저장되어있는 weight\n",
    "    local_weights = {} # 각 클라이언트에서 받아온 parameter들의 리스트\n",
    "    \n",
    "    experiment = 1 #Uniform by default\n",
    "    \n",
    "    done_clients = 0 # Task가 끝난 클라이언트의 개수\n",
    "    server_round = 0 # 현재 라운드\n",
    "    max_round = 5 #\n",
    "    total_num_data = 0 # 전체 데이터 개수\n",
    "    \n",
    "    num_data = {} \n",
    "    client_model_accuracy = {}\n",
    "    server_model_accuracy = []\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)), \n",
    "                    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'), \n",
    "                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)), \n",
    "                    tf.keras.layers.Dropout(0.25), \n",
    "                    tf.keras.layers.Flatten(), \n",
    "                    tf.keras.layers.Dense(128, activation='relu'), \n",
    "                    tf.keras.layers.Dropout(0.5), \n",
    "                    tf.keras.layers.Dense(10, activation='softmax')\n",
    "            ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    @classmethod\n",
    "    def __init__(cls):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(cls, client_num, experiment, max_round):\n",
    "        cls.client_number = client_num\n",
    "        cls.experiment = experiment\n",
    "        cls.max_round = max_round\n",
    "        cls.client_model_accuracy = {}\n",
    "        cls.reset() # reset the variables when initialized\n",
    "        return \"Initialized server\"\n",
    "    \n",
    "    @classmethod\n",
    "    def update_num_data(cls, client_id, num_data):\n",
    "        cls.total_num_data += num_data\n",
    "        cls.num_data[client_id] = num_data\n",
    "        return f\"Number of data for {client_id} updated\"\n",
    "    \n",
    "    @classmethod\n",
    "    def update(cls, client_id, local_weight):\n",
    "        local_weight = list(map(lambda weight: np.array(weight, dtype=np.float32), local_weight))\n",
    "        cls.local_weights[client_id] = local_weight\n",
    "        cls.evaluateClientModel(client_id, local_weight) \n",
    "        cls.done_clients += 1 # increment current count\n",
    "        \n",
    "        if cls.done_clients == cls.client_number: \n",
    "            cls.FedAvg() # fed avg\n",
    "            cls.evaluateServerModel()\n",
    "            cls.next_round()\n",
    "            cls.save() \n",
    "            \n",
    "        if cls.server_round == cls.max_round: # federated learning finished\n",
    "            cls.save() # save all history into json file \n",
    "            cls.reset()\n",
    "\n",
    "    @classmethod\n",
    "    def FedAvg(cls):\n",
    "        \"\"\"\n",
    "        cls.local_weights contains key:value = client id:weight array\n",
    "        \n",
    "        - At this point, we do not know the shape of the weight array, so we use np.zeros_like function to make\n",
    "        a temporary array filled with zeros, then accumulate the weights\n",
    "        \n",
    "        - The resulting weight must be a list of weights of type np.array\n",
    "        \n",
    "        - Fill in the blanks to implement FedAvg algorithm (just simple averaging)\n",
    "        \"\"\" \n",
    "        ### TODO ###\n",
    "        weight = list(map(lambda block: np.zeros_like(block, dtype=np.float32), cls.local_weights[0])) \n",
    "        # local weight와 같은 shape를 가지는 list<np.array> 를 만들기\n",
    "        \n",
    "        for client_id, client_weight in cls.local_weights.items():\n",
    "            client_num_data = cls.num_data[client_id]\n",
    "\n",
    "            for i in range(len(weight)):\n",
    "                weighted_weight = client_weight[i] * (client_num_data/cls.total_num_data)\n",
    "                weight[i] += weighted_weight\n",
    "        ### TODO ###\n",
    "        cls.set_server_weight(weight)\n",
    "        cls.evaluateServerModel()\n",
    "        \n",
    "    @classmethod\n",
    "    def evaluateClientModel(cls, client_id, weight):\n",
    "        cls.model.set_weights(cls.local_weights[client_id]) # change to local weight\n",
    "        \n",
    "        mnist = tf.keras.datasets.mnist \n",
    "        (_, _), (test_images, test_labels) = mnist.load_data()\n",
    "        n = len(test_images)\n",
    "        indices = np.random.choice([i for i in range(n)], n//10)\n",
    "        \n",
    "        test_images = test_images[indices]\n",
    "        test_labels = test_labels[indices]\n",
    "        test_images = test_images / 255\n",
    "        test_images = test_images.reshape(-1,28, 28, 1)\n",
    "        \n",
    "        acc = cls.model.evaluate(test_images, test_labels)\n",
    "        \n",
    "        if client_id not in cls.client_model_accuracy:\n",
    "            cls.client_model_accuracy[client_id] = []\n",
    "       \n",
    "        cls.client_model_accuracy[client_id].append(acc[1])\n",
    "        \n",
    "        if cls.server_weight != None:\n",
    "            cls.model.set_weights(cls.server_weight) # revert to server weight \n",
    "        \n",
    "    @classmethod\n",
    "    def evaluateServerModel(cls):\n",
    "        mnist = tf.keras.datasets.mnist \n",
    "        (_, _), (test_images, test_labels) = mnist.load_data()\n",
    "        n = len(test_images)\n",
    "        indices = np.random.choice([i for i in range(n)], n//10)\n",
    "        \n",
    "        test_images = test_images[indices]\n",
    "        test_labels = test_labels[indices]\n",
    "        test_images = test_images / 255\n",
    "        test_images = test_images.reshape(-1,28, 28, 1)\n",
    "        \n",
    "        acc = cls.model.evaluate(test_images, test_labels)[1] # first index corresponds to accuracy\n",
    "        cls.server_model_accuracy.append(acc) # each index corresponds to a round\n",
    "        \n",
    "    @classmethod\n",
    "    def next_round(cls):\n",
    "        cls.done_clients = 0 # reset current\n",
    "        cls.server_round += 1 # proceed\n",
    "        cls.total_num_data = 0 # 전체 데이터 개수 \n",
    "        cls.num_data = {} \n",
    "        \n",
    "    @classmethod\n",
    "    def save(cls):\n",
    "        result = {\"clients acc\" : cls.client_model_accuracy, \n",
    "                  \"server acc\" : cls.server_model_accuarcy}\n",
    "        import json\n",
    "        from time import gmtime, strftime\n",
    "        timestamp = strftime(\"%Y%m%d_%H%M%S\", gmtime())\n",
    "        with open(f'{timestamp}.json', 'w') as f:\n",
    "            json.dump(result, f)\n",
    "           \n",
    "        return f\"Json file saved {timestamp}\"\n",
    "\n",
    "    @classmethod\n",
    "    def reset(cls):\n",
    "        cls.client_model_accuracy = {}\n",
    "        cls.server_model_accuracy = []\n",
    "        cls.server_weight = None\n",
    "        cls.local_weights = {}\n",
    "        cls.done_clients = 0\n",
    "        cls.server_round = 0\n",
    "        cls.num_data = {}\n",
    "        cls.total_num_data = 0\n",
    "    \n",
    "    @classmethod\n",
    "    def set_server_weight(cls, weight):\n",
    "        cls.server_weight = weight\n",
    "        \n",
    "    @classmethod\n",
    "    def get_server_weight(cls):\n",
    "        return cls.server_weight\n",
    "\n",
    "    @classmethod\n",
    "    def get_done_clients(cls):\n",
    "        return cls.done_clients\n",
    "\n",
    "    @classmethod\n",
    "    def get_server_round(cls):\n",
    "        return cls.server_round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed learning on local computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_NUM = 5\n",
    "EXPERIMENT = 1\n",
    "MAX_ROUND = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "init = requests.get(f\"http://147.47.200.178:9103/initialize/{CLIENT_NUM}/{EXPERIMENT}/{MAX_ROUND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initialized server'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-43a0da3a9819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLIENT_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     client = Client(max_round =MAX_ROUND, \n\u001b[0m\u001b[1;32m      6\u001b[0m                     \u001b[0mtime_delay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Client' is not defined"
     ]
    }
   ],
   "source": [
    "clients = []\n",
    "\n",
    "\n",
    "for i in range(CLIENT_NUM):\n",
    "    client = Client(max_round =MAX_ROUND, \n",
    "                    time_delay = 5, \n",
    "                    num_samples = 100,  \n",
    "                    suppress=True, \n",
    "                    client_id = i, \n",
    "                    experiment = EXPERIMENT)\n",
    "    clients.append(client) #retain references to the clients\n",
    "\n",
    "for client in clients:\n",
    "    t = threading.Thread(target=client.task)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
