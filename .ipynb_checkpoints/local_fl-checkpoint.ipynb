{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Federated learning이란 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Django API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Django를 사용해서 Federated Learning을 위한 서버를 구축할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아래는 학습을 위한 ```Client``` 클래스이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile client.py\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "from random import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Quiet tensorflow error messages\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder): # inherits JSONEncoder \n",
    "    def default(self, o):\n",
    "        if isinstance(o, np.ndarray):\n",
    "            return o.tolist()\n",
    "        return json.JSONEncoder.default(self, o)\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, max_round: int, time_delay = 5, suppress=True, num_samples=600, client_id = 0, experiment = 1):\n",
    "        '''\n",
    "        Urls\n",
    "        '''\n",
    "        self.base_url = \"http://147.47.200.178:9103/\" # Base Url\n",
    "        self.put_weight_url =  self.base_url + \"put_weight/\" + str(client_id)\n",
    "        self.get_weight_url =  self.base_url + \"get_server_weight\" # Url that we send or fetch weight parameters\n",
    "        self.round_url =  self.base_url + \"get_server_round\" \n",
    "\n",
    "        '''\n",
    "        Initial setup\n",
    "        '''\n",
    "        self.experiment = experiment\n",
    "        self.client_id = client_id\n",
    "        self.time_delay = time_delay\n",
    "        self.suppress = suppress\n",
    "        self.global_round = self.request_global_round()\n",
    "        self.current_round = 0\n",
    "        self.max_round = max_round # Set the maximum number of rounds\n",
    "        \n",
    "        '''\n",
    "        Downloads MNIST dataset and prepares (train_x, train_y), (test_x, test_y)\n",
    "        '''\n",
    "        self.train_images, self.train_labels, self.test_images, self.test_labels = self.prepare_images()\n",
    "        self.split_train_images, self.split_train_labels = self.data_split(num_samples)\n",
    "        self.local_data_num = len(self.split_train_labels)\n",
    "        \n",
    "        '''\n",
    "        Builds model\n",
    "        '''\n",
    "        self.model = self.build_cnn_model()\n",
    "        \n",
    "    def prepare_images(self):\n",
    "        \"\"\"\n",
    "        return: \n",
    "            None : Prepares MNIST images in the required format for each model\n",
    "            \n",
    "        \"\"\"\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "        train_images, test_images = train_images / 255, test_images / 255\n",
    "        \n",
    "        # For CNN, add dummy channel to feed the images to CNN\n",
    "        train_images=train_images.reshape(-1,28, 28, 1)\n",
    "        test_images=test_images.reshape(-1,28, 28, 1)\n",
    "        return train_images, train_labels, test_images, test_labels\n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            None : saves the CNN model in self.model variable \n",
    "        \"\"\"\n",
    "        #This model definition must be same in the server (Federated.py)\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "        \n",
    "    def data_split(self, num_samples):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            num_samples : The number of sample images in each client. This value is used for equally\n",
    "                          sized dataset\n",
    "        \n",
    "        @return: \n",
    "            None : Split the dataset depending on the self.experiment value\n",
    "           \n",
    "                If self.experiment is 1: Uniform data split: We take equal amount of data from each class (iid)\n",
    "                If self.experiment is 2: Random data split1: We take equal amount of data, but not uniformly distributed across classes\n",
    "                If self.experiment is 3: Random data split2: We take different amount of data and not uniformly distributed across classes\n",
    "                If self.experiment is 4: Skewed: We take disproportionate amount of data for some classes\n",
    "                        \n",
    "        \"\"\"\n",
    "        \n",
    "        train_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "        test_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "        for i, v in enumerate(self.train_labels):\n",
    "            train_index_list[v].append(i)\n",
    "\n",
    "        for i, v in enumerate(self.test_labels):\n",
    "            test_index_list[v].append(i)\n",
    "\n",
    "        \n",
    "        split_train_images = []\n",
    "        split_train_labels = []\n",
    "        \n",
    "        \"\"\"\n",
    "        Todo : split the data according to the instructions        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        For each experiment, you must\n",
    "            1. save the total number of samples to self.local_data_num variable\n",
    "            2. add the split images and labels into self.split_train_images and self.split_train_labels respectively\n",
    "        \"\"\"\n",
    "        if self.experiment == 1: #uniform data split\n",
    "            # all \n",
    "            self.local_data_num = num_samples\n",
    "            \n",
    "            for i in range(len(train_index_list)):\n",
    "                indices = train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=num_samples//10)\n",
    "                \n",
    "                split_train_images.extend(self.train_images[random_indices])\n",
    "                split_train_labels.extend(self.train_labels[random_indices])\n",
    "            \n",
    "\n",
    "        elif self.experiment == 2: # Randomly selected, equally sized dataset\n",
    "            self.local_data_num = num_samples\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=num_samples)\n",
    "            split_train_images = self.train_images[random_indices]\n",
    "            split_train_labels = self.train_labels[random_indices]\n",
    "\n",
    "        \n",
    "            \n",
    "        elif self.experiment == 3: # Randomly selected, differently sized dataset\n",
    "            n = np.random.randint(1, num_samples)\n",
    "            self.local_data_num = n\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=n)\n",
    "            split_train_images = self.train_images[random_indices]\n",
    "            split_train_labels = self.train_labels[random_indices]\n",
    "            \n",
    "     \n",
    "  \n",
    "        elif self.experiment == 4: #Skewed\n",
    "            temp = [i for i in range(10)]\n",
    "            skewed_numbers = np.random.choice(temp, np.random.randint(1, 10))\n",
    "            non_skewed_numbers = list(set(temp)-set(skewed_numbers))\n",
    "            N = 0\n",
    "          \n",
    "            for i in skewed_numbers:\n",
    "                n = np.random.randint(50, 60)\n",
    "                N += n\n",
    "                \n",
    "                indices = train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                split_train_images.extend(self.train_images[random_indices])\n",
    "                split_train_labels.extend(self.train_labels[random_indices])\n",
    "                \n",
    "                \n",
    "            for i in non_skewed_numbers:\n",
    "                n = np.random.randint(1, 10)\n",
    "                N += n\n",
    "                \n",
    "                indices = train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                split_train_images.extend(self.train_images[random_indices])\n",
    "                split_train_labels.extend(self.train_labels[random_indices])\n",
    "      \n",
    "            \n",
    "            self.local_data_num = N\n",
    "        \n",
    "        split_train_images = np.array(split_train_images)\n",
    "        split_train_labels = np.array(split_train_labels)\n",
    "        return split_train_images, split_train_labels\n",
    "\n",
    "        \n",
    "        \n",
    "    def update_total_num_data(self, num_data):\n",
    "        \"\"\"\n",
    "        num_data : the number of training images that the current client has\n",
    "        \n",
    "        update the total number of training images that is stored in the server\n",
    "        \"\"\"\n",
    "        update_num_data_url =  self.base_url + \"update_num_data/\"+str(self.client_id)+\"/\"+str(num_data)\n",
    "        requests.get(update_num_data_url)\n",
    "        \n",
    "\n",
    "    \n",
    "    def request_global_round(self):\n",
    "        \"\"\"\n",
    "        result : Current global round that the server is in\n",
    "        \"\"\"\n",
    "        result = requests.get(self.round_url)\n",
    "        result = result.json()\n",
    "        return result\n",
    "    \n",
    "    def request_global_weight(self):\n",
    "        \"\"\"\n",
    "        global_weight : Up-to-date version of the model parameters\n",
    "        \"\"\"\n",
    "        result = requests.get(self.get_weight_url)\n",
    "        result_data = result.json()\n",
    "        \n",
    "        global_weight = None\n",
    "        if result_data is not None:\n",
    "            global_weight = []\n",
    "            for i in range(len(result_data)):\n",
    "                temp = np.array(result_data[i], dtype=np.float32)\n",
    "                global_weight.append(temp)\n",
    "            \n",
    "        return global_weight\n",
    "\n",
    "    def upload_local_weight(self, local_weight):\n",
    "        \"\"\"\n",
    "        local_weight : the local weight that current client has converged to\n",
    "        \n",
    "        Add current client's weights to the server (Server accumulates these from multiple clients and computes the global weight)\n",
    "        \"\"\"\n",
    "        local_weight_to_json = json.dumps(local_weight, cls=NumpyEncoder)\n",
    "        requests.put(self.put_weight_url, data=local_weight_to_json)\n",
    "        \n",
    "    def train_local_model(self):\n",
    "        \"\"\"\n",
    "        local_weight : local weight of the current client after training\n",
    "        \"\"\"\n",
    "        global_weight = self.request_global_weight()\n",
    "        if global_weight != None:\n",
    "            global_weight = np.array(global_weight)\n",
    "            self.model.set_weights(global_weight)\n",
    "            \n",
    "        \n",
    "        self.model.fit(self.split_train_images, self.split_train_labels, epochs=10, batch_size=8, verbose=0)\n",
    "        local_weight = self.model.get_weights()\n",
    "        return local_weight\n",
    "    \n",
    "    def task(self):\n",
    "        \"\"\"\n",
    "        Federated learning task\n",
    "        1. If the current round is larger than the max round that we set, finish\n",
    "        2. If the global round = current client's round, the client needs update\n",
    "        3. Otherwise, we need to wait until other clients to finish\n",
    "        \"\"\"\n",
    "        \n",
    "        #this is for executing on multiple devices\n",
    "        self.global_round = self.request_global_round()\n",
    "        \n",
    "        print(\"global round\", self.global_round)\n",
    "        print(\"current round\", self.current_round)\n",
    "        if self.current_round >= self.max_round:\n",
    "            print(f\"Client {self.client_id} finished\")\n",
    "            return \n",
    "\n",
    "        if self.global_round == self.current_round: #need update \n",
    "            print(\"Client \"+ str(self.client_id) + \"needs update\")\n",
    "            self.split_train_images, self.split_train_labels = self.data_split(num_samples=self.local_data_num)\n",
    "            self.update_total_num_data(self.local_data_num)        \n",
    "            local_weight = self.train_local_model()\n",
    "            self.upload_local_weight(local_weight)\n",
    "            self.current_round += 1\n",
    "            time.sleep(self.time_delay)\n",
    "            return self.task()\n",
    "\n",
    "        else: #need to wait until other clients finish\n",
    "            print(\"need wait\")\n",
    "            time.sleep(self.time_delay)\n",
    "            return self.task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting FL_Server/app/Federated.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FL_Server/app/Federated.py\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from app import numpy_encoder\n",
    "import os \n",
    "    \n",
    "class FederatedServer:\n",
    "    client_number = 5 # 전체 클라이언트 개수\n",
    "    server_weight = None # 현재 서버에 저장되어있는 weight\n",
    "    local_weights = {} # 각 클라이언트에서 받아온 parameter들의 리스트\n",
    "    \n",
    "    experiment = 1 #Uniform by default\n",
    "    \n",
    "    done_clients = 0 # Task가 끝난 클라이언트의 개수\n",
    "    server_round = 0 # 현재 라운드\n",
    "    max_round = 5 #\n",
    "    total_num_data = 0 # 전체 데이터 개수\n",
    "    \n",
    "    num_data = {} \n",
    "    client_model_accuracy = {}\n",
    "    server_model_accuracy = []\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)), \n",
    "                    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'), \n",
    "                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)), \n",
    "                    tf.keras.layers.Dropout(0.25), \n",
    "                    tf.keras.layers.Flatten(), \n",
    "                    tf.keras.layers.Dense(128, activation='relu'), \n",
    "                    tf.keras.layers.Dropout(0.5), \n",
    "                    tf.keras.layers.Dense(10, activation='softmax')\n",
    "            ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(cls, client_num, experiment, max_round):\n",
    "        cls.client_number = client_num\n",
    "        cls.experiment = experiment\n",
    "        cls.max_round = max_round\n",
    "        cls.client_model_accuracy = {}\n",
    "        cls.reset() # reset the variables when initialized\n",
    "        return \"Initialized server\"\n",
    "    \n",
    "    @classmethod\n",
    "    def update_num_data(cls, client_id, num_data):\n",
    "        cls.total_num_data += num_data\n",
    "        cls.num_data[client_id] = num_data\n",
    "        return f\"Number of data for {client_id} updated\"\n",
    "    \n",
    "    @classmethod\n",
    "    def update(cls, client_id, local_weight):\n",
    "        local_weight = list(map(lambda weight: np.array(weight, dtype=np.float32), local_weight))\n",
    "        cls.local_weights[client_id] = local_weight\n",
    "        cls.evaluateClientModel(client_id, local_weight) \n",
    "        cls.done_clients += 1 # increment current count\n",
    "        \n",
    "        if cls.done_clients == cls.client_number: \n",
    "            cls.FedAvg() # fed avg\n",
    "            cls.evaluateServerModel()\n",
    "            cls.next_round()\n",
    "            cls.save() \n",
    "            \n",
    "        if cls.server_round == cls.max_round: # federated learning finished\n",
    "            cls.save() # save all history into json file \n",
    "            cls.reset()\n",
    "\n",
    "    @classmethod\n",
    "    def FedAvg(cls):\n",
    "        \"\"\"\n",
    "        cls.local_weights contains key:value = client id:weight array\n",
    "        \n",
    "        - At this point, we do not know the shape of the weight array, so we use np.zeros_like function to make\n",
    "        a temporary array filled with zeros, then accumulate the weights\n",
    "        \n",
    "        - The resulting weight must be a list of weights of type np.array\n",
    "        \n",
    "        - Fill in the blanks to implement FedAvg algorithm (just simple averaging)\n",
    "        \"\"\" \n",
    "        ### TODO ###\n",
    "        weight = list(map(lambda block: np.zeros_like(block, dtype=np.float32), cls.local_weights[0])) \n",
    "        # local weight와 같은 shape를 가지는 list<np.array> 를 만들기\n",
    "        \n",
    "        for client_id, client_weight in cls.local_weights.items():\n",
    "            client_num_data = cls.num_data[client_id]\n",
    "\n",
    "            for i in range(len(weight)):\n",
    "                weighted_weight = client_weight[i] * (client_num_data/cls.total_num_data)\n",
    "                weight[i] += weighted_weight\n",
    "        ### TODO ###\n",
    "        cls.set_server_weight(weight)\n",
    "        cls.evaluateServerModel()\n",
    "        \n",
    "    @classmethod\n",
    "    def evaluateClientModel(cls, client_id, weight):\n",
    "        cls.model.set_weights(cls.local_weights[client_id]) # change to local weight\n",
    "        \n",
    "        mnist = tf.keras.datasets.mnist \n",
    "        (_, _), (test_images, test_labels) = mnist.load_data()\n",
    "        n = len(test_images)\n",
    "        indices = np.random.choice([i for i in range(n)], n//10)\n",
    "        \n",
    "        test_images = test_images[indices]\n",
    "        test_labels = test_labels[indices]\n",
    "        test_images = test_images / 255\n",
    "        test_images = test_images.reshape(-1,28, 28, 1)\n",
    "        \n",
    "        acc = cls.model.evaluate(test_images, test_labels)\n",
    "        \n",
    "        if client_id not in cls.client_model_accuracy:\n",
    "            cls.client_model_accuracy[client_id] = []\n",
    "       \n",
    "        cls.client_model_accuracy[client_id].append(acc[1])\n",
    "        \n",
    "        if cls.server_weight != None:\n",
    "            cls.model.set_weights(cls.server_weight) # revert to server weight \n",
    "        \n",
    "    @classmethod\n",
    "    def evaluateServerModel(cls):\n",
    "        mnist = tf.keras.datasets.mnist \n",
    "        (_, _), (test_images, test_labels) = mnist.load_data()\n",
    "        n = len(test_images)\n",
    "        indices = np.random.choice([i for i in range(n)], n//10)\n",
    "        \n",
    "        test_images = test_images[indices]\n",
    "        test_labels = test_labels[indices]\n",
    "        test_images = test_images / 255\n",
    "        test_images = test_images.reshape(-1,28, 28, 1)\n",
    "        \n",
    "        acc = cls.model.evaluate(test_images, test_labels)[1] # first index corresponds to accuracy\n",
    "        # each index corresponds to a round\n",
    "        cls.server_model_accuracy.append(acc) \n",
    "        \n",
    "    @classmethod\n",
    "    def next_round(cls):\n",
    "        cls.done_clients = 0 # reset current\n",
    "        cls.server_round += 1 # proceed\n",
    "        cls.total_num_data = 0 # 전체 데이터 개수 \n",
    "        cls.num_data = {} \n",
    "        \n",
    "    @classmethod\n",
    "    def save(cls):\n",
    "        result = {\"clients acc\" : cls.client_model_accuracy, \n",
    "                  \"server acc\" : cls.server_model_accuarcy}\n",
    "        import json\n",
    "        from time import gmtime, strftime\n",
    "        timestamp = strftime(\"%Y%m%d_%H%M%S\", gmtime())\n",
    "        with open(f'{timestamp}.json', 'w') as f:\n",
    "            json.dump(result, f)\n",
    "           \n",
    "        return f\"Json file saved {timestamp}\"\n",
    "\n",
    "    @classmethod\n",
    "    def reset(cls):\n",
    "        cls.client_model_accuracy = {}\n",
    "        cls.server_model_accuracy = []\n",
    "        cls.server_weight = None\n",
    "        cls.local_weights = {}\n",
    "        cls.done_clients = 0\n",
    "        cls.server_round = 0\n",
    "        cls.num_data = {}\n",
    "        cls.total_num_data = 0\n",
    "    \n",
    "    @classmethod\n",
    "    def set_server_weight(cls, weight):\n",
    "        cls.server_weight = weight\n",
    "        \n",
    "    @classmethod\n",
    "    def get_server_weight(cls):\n",
    "        return cls.server_weight\n",
    "\n",
    "    @classmethod\n",
    "    def get_done_clients(cls):\n",
    "        return cls.done_clients\n",
    "\n",
    "    @classmethod\n",
    "    def get_server_round(cls):\n",
    "        return cls.server_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed learning on local computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_NUM = 2\n",
    "EXPERIMENT = 1\n",
    "MAX_ROUND = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "init = requests.get(f\"http://147.47.200.178:9103/initialize/{CLIENT_NUM}/{EXPERIMENT}/{MAX_ROUND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initialized server'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global round 0\n",
      "current round 0\n",
      "Client 0needs update\n",
      "global round 0\n",
      "current round 0\n",
      "Client 1needs update\n",
      "global round 1\n",
      "current round 1\n",
      "Client 0needs update\n",
      "global round 1\n",
      "current round 1\n",
      "Client 1needs update\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-c3ed5b06cbc6>:243: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  global_weight = np.array(global_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global round 2\n",
      "current round 2\n",
      "Client 0needs update\n",
      "global round 2\n",
      "current round 2\n",
      "Client 1needs update\n",
      "global round 0\n",
      "current round 3\n",
      "Client 0 finished\n",
      "global round 0\n",
      "current round 3\n",
      "Client 1 finished\n"
     ]
    }
   ],
   "source": [
    "clients = []\n",
    "\n",
    "for i in range(CLIENT_NUM):\n",
    "    client = Client(max_round =MAX_ROUND, \n",
    "                    time_delay = 5, \n",
    "                    num_samples = 100,  \n",
    "                    suppress=True, \n",
    "                    client_id = i, \n",
    "                    experiment = EXPERIMENT)\n",
    "    clients.append(client) #retain references to the clients\n",
    "\n",
    "for client in clients:\n",
    "    thread = threading.Thread(target=client.task)\n",
    "    thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
