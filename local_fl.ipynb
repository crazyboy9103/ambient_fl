{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b2be96",
   "metadata": {},
   "source": [
    "# Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921b5e2",
   "metadata": {},
   "source": [
    "* Federated learning이란 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a6425",
   "metadata": {},
   "source": [
    "# Django API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2592e",
   "metadata": {},
   "source": [
    "* Django를 사용해서 Federated Learning을 위한 서버를 구축할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34ef68ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'absl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13868/4089484146.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Quiet tensorflow error messages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'absl'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from random import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Quiet tensorflow error messages\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder): # inherits JSONEncoder \n",
    "    def default(self, o):\n",
    "        if isinstance(o, np.ndarray):\n",
    "            return o.tolist()\n",
    "        return json.JSONEncoder.default(self, o)\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, max_round: int, time_delay = 5, suppress=True, num_samples=600, client_id = 0, experiment = 1):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            experiment : Desired data split type (1~4)\n",
    "            max_round : the maximum number of rounds that should be trained (arbitrary integer)\n",
    "            model : the NN model type (either 'ann' or 'cnn')\n",
    "            time_delay : the time delay until the next local check (arbitrary positive integer) \n",
    "                        (Need to increase this value if one round of training takes much longer than current time_delay. \n",
    "                        The reason is that any network communication until next round after the client has already uploaded \n",
    "                        the parameters for current round increases network overhead. Thus, higher time_delay will make communication\n",
    "                        more stable while increasing the absolute time it takes. Requires careful selection of this value.)\n",
    "            suppress : boolean value to print the logs\n",
    "        \n",
    "        @return: \n",
    "            None : Initializes the variables\n",
    "                   Setup the urls for communication\n",
    "                   Fetch client's id from the server\n",
    "                   Downloads MNIST dataset and splits\n",
    "                   Build model\n",
    "        \"\"\"\n",
    "        \n",
    "        '''\n",
    "        Urls\n",
    "        '''\n",
    "        \n",
    "        self.base_url = \"http://127.0.0.1:9103/\" # Base Url\n",
    "        self.put_weight_url =  self.base_url + f\"put_weight/{client_id}\"\n",
    "        self.get_weight_url =  self.base_url + \"get_server_weight\" # Url that we send or fetch weight parameters\n",
    "        self.round_url =  self.base_url + \"get_server_round\" \n",
    "\n",
    "        '''\n",
    "        Initial setup\n",
    "        '''\n",
    "        self.experiment = experiment\n",
    "        self.client_id = client_id\n",
    "        self.time_delay = time_delay\n",
    "        self.suppress = suppress\n",
    "        self.global_round = self.request_global_round()\n",
    "        self.current_round = 0\n",
    "        self.max_round = max_round # Set the maximum number of rounds\n",
    "        \n",
    "        '''\n",
    "        Downloads MNIST dataset and prepares (train_x, train_y), (test_x, test_y)\n",
    "        '''\n",
    "        self.train_images, self.train_labels = None, None\n",
    "        self.test_images, self.test_labels = None, None\n",
    "        self.prepare_images()\n",
    "        \n",
    "        self.train_index_list = None\n",
    "        self.test_index_list = None\n",
    "        self.split_train_images = []\n",
    "        self.split_train_labels = []\n",
    "\n",
    "        self.local_data_num = 0\n",
    "        \n",
    "        '''\n",
    "        Builds model\n",
    "        '''\n",
    "        self.model = None\n",
    "        self.build_cnn_model()\n",
    "        \n",
    "    def prepare_images(self):\n",
    "        \"\"\"\n",
    "        return: \n",
    "            None : Prepares MNIST images in the required format for each model\n",
    "            \n",
    "        \"\"\"\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_images, self.train_labels), (self.test_images, self.test_labels) = mnist.load_data()\n",
    "        self.train_images, self.test_images = self.train_images / 255, self.test_images / 255\n",
    "        \n",
    "        # For CNN, add dummy channel to feed the images to CNN\n",
    "        self.train_images=self.train_images.reshape(-1,28, 28, 1)\n",
    "        self.test_images=self.test_images.reshape(-1,28, 28, 1)\n",
    "            \n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            None\n",
    "        \n",
    "        @return: \n",
    "            None : saves the CNN model in self.model variable \n",
    "        \"\"\"\n",
    "        #This model definition must be same in the server (Federated.py)\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "    def data_split(self, num_samples):\n",
    "        \"\"\"\n",
    "        @params: \n",
    "            num_samples : The number of sample images in each client. This value is used for equally\n",
    "                          sized dataset\n",
    "        \n",
    "        @return: \n",
    "            None : Split the dataset depending on the self.experiment value\n",
    "           \n",
    "                If self.experiment is 1: Uniform data split: We take equal amount of data from each class (iid)\n",
    "                If self.experiment is 2: Random data split1: We take equal amount of data, but not uniformly distributed across classes\n",
    "                If self.experiment is 3: Random data split2: We take different amount of data and not uniformly distributed across classes\n",
    "                If self.experiment is 4: Skewed: We take disproportionate amount of data for some classes\n",
    "                        \n",
    "        \"\"\"\n",
    "        if self.train_index_list is None or self.test_index_list is None:\n",
    "            self.train_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "            self.test_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
    "            for i, v in enumerate(self.train_labels):\n",
    "                self.train_index_list[v].append(i)\n",
    "\n",
    "            for i, v in enumerate(self.test_labels):\n",
    "                self.test_index_list[v].append(i)\n",
    "\n",
    "        \n",
    "        self.split_train_images = []\n",
    "        self.split_train_labels = []\n",
    "        \n",
    "        \"\"\"\n",
    "        Todo : split the data according to the instructions        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        For each experiment, you must\n",
    "            1. save the total number of samples to self.local_data_num variable\n",
    "            2. add the split images and labels into self.split_train_images and self.split_train_labels respectively\n",
    "        \"\"\"\n",
    "        if self.experiment == 1: #uniform data split\n",
    "            # all \n",
    "            self.local_data_num = num_samples\n",
    "            \n",
    "            for i in range(len(self.train_index_list)):\n",
    "                indices = self.train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=num_samples//10)\n",
    "                \n",
    "                self.split_train_images.extend(self.train_images[random_indices])\n",
    "                self.split_train_labels.extend(self.train_labels[random_indices])\n",
    "            \n",
    "\n",
    "        elif self.experiment == 2: # Randomly selected, equally sized dataset\n",
    "            self.local_data_num = num_samples\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=num_samples)\n",
    "            self.split_train_images = self.train_images[random_indices]\n",
    "            self.split_train_labels = self.train_labels[random_indices]\n",
    "\n",
    "            counts = [0 for _ in range(10)]\n",
    "            \n",
    "            for label in self.train_labels[random_indices]:\n",
    "                counts[label] += 1\n",
    "            \n",
    "        elif self.experiment == 3: # Randomly selected, differently sized dataset\n",
    "            n = np.random.randint(1, num_samples)\n",
    "            self.local_data_num = n\n",
    "            random_indices = np.random.choice([i for i in range(len(self.train_labels))], size=n)\n",
    "            self.split_train_images = self.train_images[random_indices]\n",
    "            self.split_train_labels = self.train_labels[random_indices]\n",
    "            \n",
    "\n",
    "            counts = [0 for _ in range(10)]\n",
    "            \n",
    "            for label in self.train_labels[random_indices]:\n",
    "                counts[label] += 1\n",
    "  \n",
    "        elif self.experiment == 4: #Skewed\n",
    "            temp = [i for i in range(10)]\n",
    "            skewed_numbers = np.random.choice(temp, np.random.randint(1, 10))\n",
    "            non_skewed_numbers = list(set(temp)-set(skewed_numbers))\n",
    "            N = 0\n",
    "            \n",
    "            counts = [0 for _ in range(10)]\n",
    "            \n",
    "            for i in skewed_numbers:\n",
    "                n = np.random.randint(50, 60)\n",
    "                N += n\n",
    "                \n",
    "                indices = self.train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                self.split_train_images.extend(self.train_images[random_indices])\n",
    "                self.split_train_labels.extend(self.train_labels[random_indices])\n",
    "                \n",
    "                counts[i] += n\n",
    "            \n",
    "                \n",
    "            for i in non_skewed_numbers:\n",
    "                n = np.random.randint(1, 10)\n",
    "                N += n\n",
    "                \n",
    "                indices = self.train_index_list[i]\n",
    "                random_indices = np.random.choice(indices, size=n)\n",
    "                \n",
    "                self.split_train_images.extend(self.train_images[random_indices])\n",
    "                self.split_train_labels.extend(self.train_labels[random_indices])\n",
    "                \n",
    "                counts[i] += n\n",
    "            \n",
    "            self.local_data_num = N\n",
    "        \n",
    "        self.split_train_images = np.array(self.split_train_images)\n",
    "        self.split_train_labels = np.array(self.split_train_labels)\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "    def update_total_num_data(self, num_data):\n",
    "        \"\"\"\n",
    "        num_data : the number of training images that the current client has\n",
    "        \n",
    "        update the total number of training images that is stored in the server\n",
    "        \"\"\"\n",
    "        update_num_data_url =  self.base_url + f\"update_num_data/{self.client_id}/{num_data}\"\n",
    "        requests.get(update_num_data_url)\n",
    "        \n",
    "\n",
    "    \n",
    "    def request_global_round(self):\n",
    "        \"\"\"\n",
    "        result : Current global round that the server is in\n",
    "        \"\"\"\n",
    "        result = requests.get(self.round_url)\n",
    "        result = result.json()\n",
    "        return result\n",
    "    \n",
    "    def request_global_weight(self):\n",
    "        \"\"\"\n",
    "        global_weight : Up-to-date version of the model parameters\n",
    "        \"\"\"\n",
    "        result = requests.get(self.get_weight_url)\n",
    "        result_data = result.json()\n",
    "        \n",
    "        global_weight = None\n",
    "        if result_data is not None:\n",
    "            global_weight = []\n",
    "            for i in range(len(result_data)):\n",
    "                temp = np.array(result_data[i], dtype=np.float32)\n",
    "                global_weight.append(temp)\n",
    "            \n",
    "        return global_weight\n",
    "\n",
    "    def upload_local_weight(self, local_weight):\n",
    "        \"\"\"\n",
    "        local_weight : the local weight that current client has converged to\n",
    "        \n",
    "        Add current client's weights to the server (Server accumulates these from multiple clients and computes the global weight)\n",
    "        \"\"\"\n",
    "        local_weight_to_json = json.dumps(local_weight, cls=NumpyEncoder)\n",
    "        requests.put(self.put_weight_url, data=local_weight_to_json)\n",
    "        \n",
    "    def train_local_model(self):\n",
    "        \"\"\"\n",
    "        local_weight : local weight of the current client after training\n",
    "        \"\"\"\n",
    "        global_weight = self.request_global_weight()\n",
    "        if global_weight != None:\n",
    "            global_weight = np.array(global_weight)\n",
    "            self.model.set_weights(global_weight)\n",
    "            \n",
    "        \n",
    "        self.model.fit(self.split_train_images, self.split_train_labels, epochs=10, batch_size=8, verbose=0)\n",
    "        local_weight = self.model.get_weights()\n",
    "        return local_weight\n",
    "    \n",
    "    def task(self):\n",
    "        \"\"\"\n",
    "        Federated learning task\n",
    "        1. If the current round is larger than the max round that we set, finish\n",
    "        2. If the global round = current client's round, the client needs update\n",
    "        3. Otherwise, we need to wait until other clients to finish\n",
    "        \"\"\"\n",
    "        \n",
    "        #this is for executing on multiple devices\n",
    "        self.global_round = self.request_global_round()\n",
    "        \n",
    "        print(\"global round\", self.global_round)\n",
    "        print(\"current round\", self.current_round)\n",
    "        if self.current_round >= self.max_round:\n",
    "            print(f\"Client {self.client_id} finished\")\n",
    "            return \n",
    "\n",
    "        if self.global_round == self.current_round: #need update \n",
    "            self.data_split(num_samples=self.local_data_num)\n",
    "            self.update_total_num_data(self.local_data_num) \n",
    "            local_weight = self.train_local_model()\n",
    "            self.upload_local_weight(local_weight)\n",
    "            self.current_round += 1\n",
    "            time.sleep(self.time_delay)\n",
    "            return self.task()\n",
    "\n",
    "        else: #need to wait until other clients finish\n",
    "            time.sleep(self.time_delay * 2)\n",
    "            return self.task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5033fde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting FL_Server/app/Federated.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FL_Server/app/Federated.py\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from app import numpy_encoder\n",
    "import os \n",
    "    \n",
    "class FederatedServer:\n",
    "    client_number = 5 # 전체 클라이언트 개수\n",
    "    server_weight = None # 현재 서버에 저장되어있는 weight\n",
    "    local_weights = {} # 각 클라이언트에서 받아온 parameter들의 리스트\n",
    "    \n",
    "    experiment = 1 #Uniform by default\n",
    "    \n",
    "    done_clients = 0 # Task가 끝난 클라이언트의 개수\n",
    "    server_round = 0 # 현재 라운드\n",
    "    max_round = 5 #\n",
    "    total_num_data = 0 # 전체 데이터 개수\n",
    "    \n",
    "    num_data = {} \n",
    "    client_model_accuracy = {}\n",
    "    server_model_accuracy = []\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)), \n",
    "                    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'), \n",
    "                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)), \n",
    "                    tf.keras.layers.Dropout(0.25), \n",
    "                    tf.keras.layers.Flatten(), \n",
    "                    tf.keras.layers.Dense(128, activation='relu'), \n",
    "                    tf.keras.layers.Dropout(0.5), \n",
    "                    tf.keras.layers.Dense(10, activation='softmax')\n",
    "            ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    @classmethod\n",
    "    def __init__(cls):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(cls, client_num, experiment, max_round):\n",
    "        cls.client_number = client_num\n",
    "        cls.experiment = experiment\n",
    "        cls.max_round = max_round\n",
    "        cls.client_model_accuracy = {}\n",
    "        cls.reset() # reset the variables when initialized\n",
    "        return \"Initialized server\"\n",
    "    \n",
    "    @classmethod\n",
    "    def update_num_data(cls, client_id, num_data):\n",
    "        cls.total_num_data += num_data\n",
    "        cls.num_data[client_id] = num_data\n",
    "        return f\"Number of data for {client_id} updated\"\n",
    "    \n",
    "    @classmethod\n",
    "    def update(cls, client_id, local_weight):\n",
    "        local_weight = list(map(lambda weight: np.array(weight, dtype=np.float32), local_weight))\n",
    "        cls.local_weights[client_id] = local_weight\n",
    "        cls.evaluateClientModel(client_id, local_weight) \n",
    "        cls.done_clients += 1 # increment current count\n",
    "        \n",
    "        if cls.done_clients == cls.client_number: \n",
    "            cls.FedAvg() # fed avg\n",
    "            cls.evaluateServerModel()\n",
    "            cls.next_round()\n",
    "            cls.save() \n",
    "            \n",
    "        if cls.server_round == cls.max_round: # federated learning finished\n",
    "            cls.save() # save all history into json file \n",
    "            cls.reset()\n",
    "\n",
    "    @classmethod\n",
    "    def FedAvg(cls):\n",
    "        \"\"\"\n",
    "        cls.local_weights contains key:value = client id:weight array\n",
    "        \n",
    "        - At this point, we do not know the shape of the weight array, so we use np.zeros_like function to make\n",
    "        a temporary array filled with zeros, then accumulate the weights\n",
    "        \n",
    "        - The resulting weight must be a list of weights of type np.array\n",
    "        \n",
    "        - Fill in the blanks to implement FedAvg algorithm (just simple averaging)\n",
    "        \"\"\" \n",
    "        ### TODO ###\n",
    "        weight = list(map(lambda block: np.zeros_like(block, dtype=np.float32), cls.local_weights[0])) \n",
    "        # local weight와 같은 shape를 가지는 list<np.array> 를 만들기\n",
    "        \n",
    "        for client_id, client_weight in cls.local_weights.items():\n",
    "            client_num_data = cls.num_data[client_id]\n",
    "\n",
    "            for i in range(len(weight)):\n",
    "                weighted_weight = client_weight[i] * (client_num_data/cls.total_num_data)\n",
    "                weight[i] += weighted_weight\n",
    "        ### TODO ###\n",
    "        cls.set_server_weight(weight)\n",
    "        cls.evaluateServerModel()\n",
    "        \n",
    "    @classmethod\n",
    "    def evaluateClientModel(cls, client_id, weight):\n",
    "        cls.model.set_weights(cls.local_weights[client_id]) # change to local weight\n",
    "        \n",
    "        mnist = tf.keras.datasets.mnist \n",
    "        (_, _), (test_images, test_labels) = mnist.load_data()\n",
    "        n = len(test_images)\n",
    "        indices = np.random.choice([i for i in range(n)], n//10)\n",
    "        \n",
    "        test_images = test_images[indices]\n",
    "        test_labels = test_labels[indices]\n",
    "        test_images = test_images / 255\n",
    "        test_images = test_images.reshape(-1,28, 28, 1)\n",
    "        \n",
    "        acc = cls.model.evaluate(test_images, test_labels)\n",
    "        \n",
    "        if client_id not in cls.client_model_accuracy:\n",
    "            cls.client_model_accuracy[client_id] = []\n",
    "       \n",
    "        cls.client_model_accuracy[client_id].append(acc[1])\n",
    "        \n",
    "        if cls.server_weight != None:\n",
    "            cls.model.set_weights(cls.server_weight) # revert to server weight \n",
    "        \n",
    "    @classmethod\n",
    "    def evaluateServerModel(cls):\n",
    "        mnist = tf.keras.datasets.mnist \n",
    "        (_, _), (test_images, test_labels) = mnist.load_data()\n",
    "        n = len(test_images)\n",
    "        indices = np.random.choice([i for i in range(n)], n//10)\n",
    "        \n",
    "        test_images = test_images[indices]\n",
    "        test_labels = test_labels[indices]\n",
    "        test_images = test_images / 255\n",
    "        test_images = test_images.reshape(-1,28, 28, 1)\n",
    "        \n",
    "        acc = cls.model.evaluate(test_images, test_labels)[1] # first index corresponds to accuracy\n",
    "        cls.server_model_accuracy.append(acc) # each index corresponds to a round\n",
    "        \n",
    "    @classmethod\n",
    "    def next_round(cls):\n",
    "        cls.done_clients = 0 # reset current\n",
    "        cls.server_round += 1 # proceed\n",
    "        cls.total_num_data = 0 # 전체 데이터 개수 \n",
    "        cls.num_data = {} \n",
    "        \n",
    "    @classmethod\n",
    "    def save(cls):\n",
    "        result = {\"clients acc\" : cls.client_model_accuracy, \n",
    "                  \"server acc\" : cls.server_model_accuarcy}\n",
    "        import json\n",
    "        from time import gmtime, strftime\n",
    "        timestamp = strftime(\"%Y%m%d_%H%M%S\", gmtime())\n",
    "        with open(f'{timestamp}.json', 'w') as f:\n",
    "            json.dump(result, f)\n",
    "           \n",
    "        return f\"Json file saved {timestamp}\"\n",
    "\n",
    "    @classmethod\n",
    "    def reset(cls):\n",
    "        cls.client_model_accuracy = {}\n",
    "        cls.server_model_accuracy = []\n",
    "        cls.server_weight = None\n",
    "        cls.local_weights = {}\n",
    "        cls.done_clients = 0\n",
    "        cls.server_round = 0\n",
    "        cls.num_data = {}\n",
    "        cls.total_num_data = 0\n",
    "    \n",
    "    @classmethod\n",
    "    def set_server_weight(cls, weight):\n",
    "        cls.server_weight = weight\n",
    "        \n",
    "    @classmethod\n",
    "    def get_server_weight(cls):\n",
    "        return cls.server_weight\n",
    "\n",
    "    @classmethod\n",
    "    def get_done_clients(cls):\n",
    "        return cls.done_clients\n",
    "\n",
    "    @classmethod\n",
    "    def get_server_round(cls):\n",
    "        return cls.server_round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d8af5",
   "metadata": {},
   "source": [
    "# Fed learning on local computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd25efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_NUM = 5\n",
    "EXPERIMENT = 1\n",
    "MAX_ROUND = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe58fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "init = requests.get(f\"http://localhost:9103/initialize/{CLIENT_NUM}/{EXPERIMENT}/{MAX_ROUND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba1384bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initialized server'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "461cb30b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13868/3956358370.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCLIENT_NUM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     client = Client(max_round =MAX_ROUND, \n\u001b[0m\u001b[0;32m      6\u001b[0m                     \u001b[0mtime_delay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Client' is not defined"
     ]
    }
   ],
   "source": [
    "clients = []\n",
    "\n",
    "\n",
    "for i in range(CLIENT_NUM):\n",
    "    client = Client(max_round =MAX_ROUND, \n",
    "                    time_delay = 5, \n",
    "                    num_samples = 100,  \n",
    "                    suppress=True, \n",
    "                    client_id = i, \n",
    "                    experiment = EXPERIMENT)\n",
    "    clients.append(client) #retain references to the clients\n",
    "\n",
    "for client in clients:\n",
    "    t = threading.Thread(target=client.task)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906d4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
